{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manu-Sanchez/Learning/blob/ai%2Fbasic-gan/NLP_using_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuY85tTHHlDd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y86ddXjDKDzb",
        "outputId": "5be63e3c-f110-4b16-96b0-5c52ab203f1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5445609\n",
            "\n",
            "                     1\n",
            "  From fairest creatures we desire increase,\n",
            "  That thereby beauty's rose might never die,\n",
            "  But as the riper should by time decease,\n",
            "  His tender heir might bear his memory:\n",
            "  But thou contracted to thine own bright eyes,\n",
            "  Feed'st thy light's flame with self-substantial fuel,\n",
            "  Making a famine where abundance lies,\n",
            "  Thy self thy foe, to thy sweet self too cruel:\n",
            "  Thou that art now the world's fresh ornament,\n",
            "  And only herald to the gaudy spring,\n",
            "  Within thine own bud buriest thy content,\n",
            "  And tender churl mak'st waste in niggarding:\n",
            "    Pity the world, or else this glutton be,\n",
            "    To eat the world's due, by the grave and thee.\n",
            "\n",
            "\n",
            "                     2\n",
            "  When forty winters shall besiege thy brow,\n",
            "  And dig deep trenches in thy beauty's field,\n",
            "  Thy youth's proud livery so gazed on now,\n",
            "  Will be a tattered weed of small worth held:  \n",
            "  Then being asked, where all thy beauty lies,\n",
            "  Where all the treasure of thy lusty days;\n",
            "  To say within thine own deep su\n"
          ]
        }
      ],
      "source": [
        "with open(\"shakespeare.txt\", 'r', encoding=\"utf8\") as file:\n",
        "  text = file.read()\n",
        "\n",
        "print(len(text))\n",
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3uB7q7WKfeR",
        "outputId": "fc02e158-02fb-4124-985d-1d00baf84a58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "84\n",
            "{'_', '8', 'j', 'h', 'J', 'T', 'U', 'x', 'X', 'G', 'N', 'e', 'H', 'f', ')', 's', 'i', 'B', 'S', 'n', '&', \"'\", '5', 'F', 'O', '<', '[', 'A', 'V', ',', '?', 'q', 'z', 'g', '6', '\"', 'D', 'C', 'K', 't', '9', ']', 'W', '0', '4', '1', '>', 'R', '.', '|', '`', ';', 'b', 'Q', 'P', 'c', 'l', 'p', '(', 'w', '!', 'Z', 'd', 'o', '}', 'L', ':', 'm', 'y', 'k', 'a', 'Y', 'I', '7', '\\n', 'r', 'v', ' ', 'u', '-', 'M', '2', 'E', '3'}\n"
          ]
        }
      ],
      "source": [
        "all_characters = set(text) #Return a list of unique characters\n",
        "print(len(all_characters))\n",
        "print(all_characters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnLKaQKFKqVX"
      },
      "outputs": [],
      "source": [
        "decoder = dict(enumerate(all_characters))\n",
        "encoder = {char: idx for idx, char in decoder.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTyu_IyHLAoR",
        "outputId": "3c3f2a74-50ae-4329-cd1c-89f723aee3a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: '_', 1: '8', 2: 'j', 3: 'h', 4: 'J', 5: 'T', 6: 'U', 7: 'x', 8: 'X', 9: 'G', 10: 'N', 11: 'e', 12: 'H', 13: 'f', 14: ')', 15: 's', 16: 'i', 17: 'B', 18: 'S', 19: 'n', 20: '&', 21: \"'\", 22: '5', 23: 'F', 24: 'O', 25: '<', 26: '[', 27: 'A', 28: 'V', 29: ',', 30: '?', 31: 'q', 32: 'z', 33: 'g', 34: '6', 35: '\"', 36: 'D', 37: 'C', 38: 'K', 39: 't', 40: '9', 41: ']', 42: 'W', 43: '0', 44: '4', 45: '1', 46: '>', 47: 'R', 48: '.', 49: '|', 50: '`', 51: ';', 52: 'b', 53: 'Q', 54: 'P', 55: 'c', 56: 'l', 57: 'p', 58: '(', 59: 'w', 60: '!', 61: 'Z', 62: 'd', 63: 'o', 64: '}', 65: 'L', 66: ':', 67: 'm', 68: 'y', 69: 'k', 70: 'a', 71: 'Y', 72: 'I', 73: '7', 74: '\\n', 75: 'r', 76: 'v', 77: ' ', 78: 'u', 79: '-', 80: 'M', 81: '2', 82: 'E', 83: '3'}\n",
            "{'_': 0, '8': 1, 'j': 2, 'h': 3, 'J': 4, 'T': 5, 'U': 6, 'x': 7, 'X': 8, 'G': 9, 'N': 10, 'e': 11, 'H': 12, 'f': 13, ')': 14, 's': 15, 'i': 16, 'B': 17, 'S': 18, 'n': 19, '&': 20, \"'\": 21, '5': 22, 'F': 23, 'O': 24, '<': 25, '[': 26, 'A': 27, 'V': 28, ',': 29, '?': 30, 'q': 31, 'z': 32, 'g': 33, '6': 34, '\"': 35, 'D': 36, 'C': 37, 'K': 38, 't': 39, '9': 40, ']': 41, 'W': 42, '0': 43, '4': 44, '1': 45, '>': 46, 'R': 47, '.': 48, '|': 49, '`': 50, ';': 51, 'b': 52, 'Q': 53, 'P': 54, 'c': 55, 'l': 56, 'p': 57, '(': 58, 'w': 59, '!': 60, 'Z': 61, 'd': 62, 'o': 63, '}': 64, 'L': 65, ':': 66, 'm': 67, 'y': 68, 'k': 69, 'a': 70, 'Y': 71, 'I': 72, '7': 73, '\\n': 74, 'r': 75, 'v': 76, ' ': 77, 'u': 78, '-': 79, 'M': 80, '2': 81, 'E': 82, '3': 83}\n"
          ]
        }
      ],
      "source": [
        "print(decoder)\n",
        "print(encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQjAsJllLKfw",
        "outputId": "ea4c8073-6794-46e5-ae62-a1273cef8ddf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[74 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 45 74\n",
            " 77 77 23 75 63 67 77 13 70 16 75 11 15 39 77 55 75 11 70 39 78 75 11 15\n",
            " 77 59 11 77 62 11 15 16 75 11 77 16 19 55 75 11 70 15 11 29 74 77 77  5\n",
            "  3 70 39 77 39  3 11 75 11 52 68 77 52 11 70 78 39 68 21 15 77 75 63 15\n",
            " 11 77 67 16 33  3 39 77 19 11 76 11 75 77 62 16 11 29 74 77 77 17 78 39\n",
            " 77 70 15 77 39  3 11 77 75 16 57 11 75 77 15  3 63 78 56 62 77 52 68 77\n",
            " 39 16 67 11 77 62 11 55 11 70 15 11 29 74 77 77 12 16 15 77 39 11 19 62\n",
            " 11 75 77  3 11 16 75 77 67 16 33  3 39 77 52 11 70 75 77  3 16 15 77 67\n",
            " 11 67 63 75 68 66 74 77 77 17 78 39 77 39  3 63 78 77 55 63 19 39 75 70\n",
            " 55 39 11 62 77 39 63 77 39  3 16 19 11 77 63 59 19 77 52 75 16 33  3 39\n",
            " 77 11 68 11 15 29 74 77 77 23 11 11 62 21 15 39 77 39  3 68 77 56 16 33\n",
            "  3 39 21 15 77 13 56 70 67 11 77 59 16 39  3 77 15 11 56 13 79 15 78 52\n",
            " 15 39 70 19 39 16 70 56 77 13 78 11 56 29 74 77 77 80 70 69 16 19 33 77\n",
            " 70 77 13 70 67 16 19 11 77 59  3 11 75 11 77 70 52 78 19 62 70 19 55 11\n",
            " 77 56 16 11 15 29 74 77 77  5  3 68 77 15 11 56 13 77 39  3 68 77 13 63\n",
            " 11 29 77 39 63 77 39  3 68 77 15 59 11 11 39 77 15 11 56 13 77 39 63 63\n",
            " 77 55 75 78 11 56 66 74 77 77  5  3 63 78 77 39  3 70 39 77 70 75 39 77\n",
            " 19 63 59 77 39  3 11 77 59 63 75 56 62 21 15 77 13 75 11 15  3 77 63 75\n",
            " 19 70 67 11 19 39 29 74 77 77 27 19 62 77 63 19 56 68 77  3 11 75 70 56\n",
            " 62 77 39 63 77 39  3 11 77 33 70 78 62 68 77 15 57 75 16 19 33 29 74 77\n",
            " 77 42 16 39  3 16 19 77 39  3 16 19 11 77 63 59 19 77 52 78]\n"
          ]
        }
      ],
      "source": [
        "encoded_text = np.array([encoder[char] for char in text])\n",
        "print(encoded_text[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7goZUI-2fczg",
        "outputId": "664eb09f-b100-4bbd-9a82-e6915bb1e968"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 1, 2])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "np.arange(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEvHpLk5hcS-",
        "outputId": "731bf58b-39d6-473a-da20-baf504b42686"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5445609"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoded_text.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTIx1R5Be5Bu",
        "outputId": "da4bd5b8-faf1-4e60-cf5d-58879ed0c1e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "def one_hot_encode(encoded_text, num_different_items):\n",
        "\n",
        "  encoded_text_size = encoded_text.size\n",
        "  one_hot_encoding = np.zeros((encoded_text_size, num_different_items)).astype(np.float32)\n",
        "\n",
        "  one_hot_encoding[np.arange(encoded_text_size), encoded_text.flatten()] = 1\n",
        "\n",
        "  return one_hot_encoding.reshape((*encoded_text.shape, num_different_items))\n",
        "\n",
        "one_hot_encoded_text = one_hot_encode(encoded_text, len(all_characters))\n",
        "print(one_hot_encoded_text[0:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DXyZCPuyzXP",
        "outputId": "7877202d-8d22-4399-b3e8-e3e8684df24c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "544560"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = len(one_hot_encoded_text)//10\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZfvwHVWqO5F",
        "outputId": "2f8c24bf-94cd-42e5-c963-15e24280e12f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "one_hot_encoded_text[:x*10].reshape((10, -1))[:10, :40]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1MwJzfnuILw"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Windowed Data\n",
        "\n",
        "X -> [H, e, l, l, o, , W, o, r, l]\n",
        "Y -> [e, l, l, o, , W, o, r, l, d]\n",
        "\n",
        "We can see that the actual Y is X shifted 1 element\n",
        "\"\"\"\n",
        "def generate_batches(encoded_text, batch_size=10, seq_len=50):\n",
        "\n",
        "  #Calculate how many characters can we add into a batch\n",
        "  char_per_batch = seq_len * batch_size\n",
        "\n",
        "  #Calculate how many batches we have available\n",
        "  num_batches_avail = len(encoded_text) // char_per_batch\n",
        "\n",
        "  #Cut off the remaining characters that doesn't fit well with the actual character per batch size\n",
        "  encoded_text = encoded_text[:num_batches_avail*char_per_batch]\n",
        "\n",
        "  encoded_text = encoded_text.reshape((batch_size, -1))\n",
        "\n",
        "  for n in range(0, encoded_text.shape[1], seq_len):\n",
        "\n",
        "    x = encoded_text[:, n:n+seq_len]\n",
        "    y = np.zeros_like(x)\n",
        "\n",
        "    try:\n",
        "      y[:, :-1] = x[:, 1:]\n",
        "      y[:, -1] = encoded_text[:, n+seq_len]\n",
        "\n",
        "    except:\n",
        "      y[:, :-1] = x[:, 1:]\n",
        "      y[:, -1] = encoded_text[:, 0]\n",
        "\n",
        "\n",
        "    yield x,y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjEpnqi91Dp_"
      },
      "outputs": [],
      "source": [
        "batch_generator = generate_batches(encoded_text, batch_size=5, seq_len=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUxqko-q1LIY",
        "outputId": "1e1e1a68-f998-4606-d9b5-8cd3bb0e8831"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X: [74 77 77 77 77] Y: [77 77 77 77 77]\n",
            "X: [ 5 82 36 77 37] Y: [82 36 77 37 24]\n",
            "X: [56 56 21 62 51] Y: [56 21 62 51 74]\n",
            "X: [65  5 12 27 61] Y: [ 5 12 27 61 27]\n",
            "X: [77 77 74 77 77] Y: [77 74 77 77 77]\n"
          ]
        }
      ],
      "source": [
        "x, y = next(batch_generator)\n",
        "for x_itm, y_itm in zip(x,y):\n",
        "  print(f\"X: {x_itm} Y: {y_itm}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2Crhfn12KQK"
      },
      "outputs": [],
      "source": [
        "class CharModel(nn.Module):\n",
        "\n",
        "  def __init__(self, all_chars, num_hidden=256, num_layers=4, dropout=.5, use_gpu=False):\n",
        "    super().__init__()\n",
        "\n",
        "    self.dropout = dropout\n",
        "    self.num_layers = num_layers\n",
        "    self.num_hidden = num_hidden\n",
        "    self.use_gpu = use_gpu\n",
        "\n",
        "    self.all_chars = all_chars\n",
        "    self.decoder = dict(enumerate(all_chars))\n",
        "    self.encoder = {char: idx for idx, char in decoder.items()}\n",
        "\n",
        "    self.lstm = nn.LSTM(len(self.all_chars), num_hidden, num_layers, dropout=dropout, batch_first=True) #Batch first is because we will send data in the format (batch, seq, feature)\n",
        "    self.dropout_layer = nn.Dropout(dropout)\n",
        "    self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))\n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "\n",
        "    lstm_out, hidden = self.lstm(x, hidden)\n",
        "    drop_out = self.dropout_layer(lstm_out)\n",
        "    drop_out = drop_out.contiguous().view(-1, self.num_hidden)\n",
        "    final_out = self.fc_linear(drop_out)\n",
        "\n",
        "    return final_out, hidden\n",
        "\n",
        "  def hidden_state(self, batch_size):\n",
        "    if self.use_gpu:\n",
        "      hidden = (\n",
        "        torch.zeros(self.num_layers, batch_size, self.num_hidden).cuda(),\n",
        "        torch.zeros(self.num_layers, batch_size, self.num_hidden).cuda()\n",
        "      )\n",
        "\n",
        "    else:\n",
        "      hidden = (\n",
        "        torch.zeros(self.num_layers, batch_size, self.num_hidden),\n",
        "        torch.zeros(self.num_layers, batch_size, self.num_hidden)\n",
        "      )\n",
        "\n",
        "    return hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8aCNaOA2XPF",
        "outputId": "f66eb815-16eb-4058-faef-6e580f5de8fe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CharModel(\n",
              "  (lstm): LSTM(84, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
              "  (dropout_layer): Dropout(p=0.5, inplace=False)\n",
              "  (fc_linear): Linear(in_features=512, out_features=84, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = CharModel(all_chars=all_characters, num_hidden=512, num_layers=3, dropout=.5, use_gpu=True)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQtR3qrm4j-8",
        "outputId": "4b769acf-7510-461e-a394-c83ab17df07e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5470292"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "total_params = sum([int(param.numel()) for param in model.parameters()])\n",
        "total_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0wuO5Oo47ri"
      },
      "outputs": [],
      "source": [
        "optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1SccEeK5FIL",
        "outputId": "7fd708fb-32fb-40b7-a6cd-eb2c39c662b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training items: 4901048\n",
            "Validation items 544561\n"
          ]
        }
      ],
      "source": [
        "train_percent = .9\n",
        "train_size = int(train_percent * len(encoded_text))\n",
        "train_data = encoded_text[:train_size]\n",
        "validation_data = encoded_text[train_size:]\n",
        "\n",
        "print(\n",
        "    f\"Training items: {len(train_data)}\" + \"\\n\" + \\\n",
        "    f\"Validation items {len(validation_data)}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ca_ARFVQ5uV1"
      },
      "outputs": [],
      "source": [
        "epochs = 60\n",
        "batch_size = 100\n",
        "\n",
        "seq_len = 100\n",
        "tracker = 0\n",
        "num_char = max(encoded_text)+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLFsoTJW57Ur",
        "outputId": "a7573b90-6251-489d-985b-e2108c7e1755"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 Step: 25 Val loss: 3.2068777084350586\n",
            "Epoch 0 Step: 50 Val loss: 3.19425368309021\n",
            "Epoch 0 Step: 75 Val loss: 3.1957879066467285\n",
            "Epoch 0 Step: 100 Val loss: 3.181749105453491\n",
            "Epoch 0 Step: 125 Val loss: 3.0859487056732178\n",
            "Epoch 0 Step: 150 Val loss: 2.98827862739563\n",
            "Epoch 0 Step: 175 Val loss: 2.8832807540893555\n",
            "Epoch 0 Step: 200 Val loss: 2.7572391033172607\n",
            "Epoch 0 Step: 225 Val loss: 2.683788537979126\n",
            "Epoch 0 Step: 250 Val loss: 2.599677085876465\n",
            "Epoch 0 Step: 275 Val loss: 2.478297233581543\n",
            "Epoch 0 Step: 300 Val loss: 2.3588197231292725\n",
            "Epoch 0 Step: 325 Val loss: 2.2933597564697266\n",
            "Epoch 0 Step: 350 Val loss: 2.2380192279815674\n",
            "Epoch 0 Step: 375 Val loss: 2.196803569793701\n",
            "Epoch 0 Step: 400 Val loss: 2.1656386852264404\n",
            "Epoch 0 Step: 425 Val loss: 2.1348018646240234\n",
            "Epoch 0 Step: 450 Val loss: 2.1135993003845215\n",
            "Epoch 0 Step: 475 Val loss: 2.0704338550567627\n",
            "Epoch 1 Step: 500 Val loss: 2.049184799194336\n",
            "Epoch 1 Step: 525 Val loss: 2.0268375873565674\n",
            "Epoch 1 Step: 550 Val loss: 2.005401134490967\n",
            "Epoch 1 Step: 575 Val loss: 1.9826467037200928\n",
            "Epoch 1 Step: 600 Val loss: 1.9646564722061157\n",
            "Epoch 1 Step: 625 Val loss: 1.9501742124557495\n",
            "Epoch 1 Step: 650 Val loss: 1.923788070678711\n",
            "Epoch 1 Step: 675 Val loss: 1.9169713258743286\n",
            "Epoch 1 Step: 700 Val loss: 1.9017553329467773\n",
            "Epoch 1 Step: 725 Val loss: 1.8846484422683716\n",
            "Epoch 1 Step: 750 Val loss: 1.8676060438156128\n",
            "Epoch 1 Step: 775 Val loss: 1.8504754304885864\n",
            "Epoch 1 Step: 800 Val loss: 1.8326339721679688\n",
            "Epoch 1 Step: 825 Val loss: 1.8332198858261108\n",
            "Epoch 1 Step: 850 Val loss: 1.8126273155212402\n",
            "Epoch 1 Step: 875 Val loss: 1.8019789457321167\n",
            "Epoch 1 Step: 900 Val loss: 1.7925859689712524\n",
            "Epoch 1 Step: 925 Val loss: 1.7820539474487305\n",
            "Epoch 1 Step: 950 Val loss: 1.7766441106796265\n",
            "Epoch 1 Step: 975 Val loss: 1.758522629737854\n",
            "Epoch 2 Step: 1000 Val loss: 1.7676876783370972\n",
            "Epoch 2 Step: 1025 Val loss: 1.7489745616912842\n",
            "Epoch 2 Step: 1050 Val loss: 1.735281229019165\n",
            "Epoch 2 Step: 1075 Val loss: 1.7273873090744019\n",
            "Epoch 2 Step: 1100 Val loss: 1.7225154638290405\n",
            "Epoch 2 Step: 1125 Val loss: 1.712205410003662\n",
            "Epoch 2 Step: 1150 Val loss: 1.699578881263733\n",
            "Epoch 2 Step: 1175 Val loss: 1.7023613452911377\n",
            "Epoch 2 Step: 1200 Val loss: 1.69597327709198\n",
            "Epoch 2 Step: 1225 Val loss: 1.6813154220581055\n",
            "Epoch 2 Step: 1250 Val loss: 1.6809877157211304\n",
            "Epoch 2 Step: 1275 Val loss: 1.6776033639907837\n",
            "Epoch 2 Step: 1300 Val loss: 1.6765375137329102\n",
            "Epoch 2 Step: 1325 Val loss: 1.6773121356964111\n",
            "Epoch 2 Step: 1350 Val loss: 1.659135103225708\n",
            "Epoch 2 Step: 1375 Val loss: 1.6643742322921753\n",
            "Epoch 2 Step: 1400 Val loss: 1.6455334424972534\n",
            "Epoch 2 Step: 1425 Val loss: 1.6478766202926636\n",
            "Epoch 2 Step: 1450 Val loss: 1.6472697257995605\n",
            "Epoch 3 Step: 1475 Val loss: 1.6440151929855347\n",
            "Epoch 3 Step: 1500 Val loss: 1.6292856931686401\n",
            "Epoch 3 Step: 1525 Val loss: 1.6360305547714233\n",
            "Epoch 3 Step: 1550 Val loss: 1.617342233657837\n",
            "Epoch 3 Step: 1575 Val loss: 1.624184489250183\n",
            "Epoch 3 Step: 1600 Val loss: 1.6146039962768555\n",
            "Epoch 3 Step: 1625 Val loss: 1.6152952909469604\n",
            "Epoch 3 Step: 1650 Val loss: 1.6213769912719727\n",
            "Epoch 3 Step: 1675 Val loss: 1.605547308921814\n",
            "Epoch 3 Step: 1700 Val loss: 1.6005101203918457\n",
            "Epoch 3 Step: 1725 Val loss: 1.595700740814209\n",
            "Epoch 3 Step: 1750 Val loss: 1.594409465789795\n",
            "Epoch 3 Step: 1775 Val loss: 1.5984134674072266\n",
            "Epoch 3 Step: 1800 Val loss: 1.6005947589874268\n",
            "Epoch 3 Step: 1825 Val loss: 1.5957396030426025\n",
            "Epoch 3 Step: 1850 Val loss: 1.5926918983459473\n",
            "Epoch 3 Step: 1875 Val loss: 1.5999799966812134\n",
            "Epoch 3 Step: 1900 Val loss: 1.5829702615737915\n",
            "Epoch 3 Step: 1925 Val loss: 1.5905019044876099\n",
            "Epoch 3 Step: 1950 Val loss: 1.5760366916656494\n",
            "Epoch 4 Step: 1975 Val loss: 1.5663551092147827\n",
            "Epoch 4 Step: 2000 Val loss: 1.5700788497924805\n",
            "Epoch 4 Step: 2025 Val loss: 1.5784975290298462\n",
            "Epoch 4 Step: 2050 Val loss: 1.5642743110656738\n",
            "Epoch 4 Step: 2075 Val loss: 1.5706665515899658\n",
            "Epoch 4 Step: 2100 Val loss: 1.5678026676177979\n",
            "Epoch 4 Step: 2125 Val loss: 1.5677739381790161\n",
            "Epoch 4 Step: 2150 Val loss: 1.56453537940979\n",
            "Epoch 4 Step: 2175 Val loss: 1.569271445274353\n",
            "Epoch 4 Step: 2200 Val loss: 1.5622988939285278\n",
            "Epoch 4 Step: 2225 Val loss: 1.5517326593399048\n",
            "Epoch 4 Step: 2250 Val loss: 1.558529019355774\n",
            "Epoch 4 Step: 2275 Val loss: 1.556946039199829\n",
            "Epoch 4 Step: 2300 Val loss: 1.5502976179122925\n",
            "Epoch 4 Step: 2325 Val loss: 1.5515542030334473\n",
            "Epoch 4 Step: 2350 Val loss: 1.5500156879425049\n",
            "Epoch 4 Step: 2375 Val loss: 1.5477508306503296\n",
            "Epoch 4 Step: 2400 Val loss: 1.5638114213943481\n",
            "Epoch 4 Step: 2425 Val loss: 1.548307180404663\n",
            "Epoch 4 Step: 2450 Val loss: 1.539555549621582\n",
            "Epoch 5 Step: 2475 Val loss: 1.5561378002166748\n",
            "Epoch 5 Step: 2500 Val loss: 1.5482960939407349\n",
            "Epoch 5 Step: 2525 Val loss: 1.5500328540802002\n",
            "Epoch 5 Step: 2550 Val loss: 1.533007025718689\n",
            "Epoch 5 Step: 2575 Val loss: 1.536340594291687\n",
            "Epoch 5 Step: 2600 Val loss: 1.5408324003219604\n",
            "Epoch 5 Step: 2625 Val loss: 1.5555263757705688\n",
            "Epoch 5 Step: 2650 Val loss: 1.5300688743591309\n",
            "Epoch 5 Step: 2675 Val loss: 1.5291259288787842\n",
            "Epoch 5 Step: 2700 Val loss: 1.5269396305084229\n",
            "Epoch 5 Step: 2725 Val loss: 1.5530539751052856\n",
            "Epoch 5 Step: 2750 Val loss: 1.5277854204177856\n",
            "Epoch 5 Step: 2775 Val loss: 1.537617802619934\n",
            "Epoch 5 Step: 2800 Val loss: 1.5383620262145996\n",
            "Epoch 5 Step: 2825 Val loss: 1.5329079627990723\n",
            "Epoch 5 Step: 2850 Val loss: 1.5264636278152466\n",
            "Epoch 5 Step: 2875 Val loss: 1.532211422920227\n",
            "Epoch 5 Step: 2900 Val loss: 1.5389642715454102\n",
            "Epoch 5 Step: 2925 Val loss: 1.5226070880889893\n",
            "Epoch 6 Step: 2950 Val loss: 1.5122731924057007\n",
            "Epoch 6 Step: 2975 Val loss: 1.5226513147354126\n",
            "Epoch 6 Step: 3000 Val loss: 1.5350311994552612\n",
            "Epoch 6 Step: 3025 Val loss: 1.521101713180542\n",
            "Epoch 6 Step: 3050 Val loss: 1.5265883207321167\n",
            "Epoch 6 Step: 3075 Val loss: 1.5239650011062622\n",
            "Epoch 6 Step: 3100 Val loss: 1.5105695724487305\n",
            "Epoch 6 Step: 3125 Val loss: 1.5237820148468018\n",
            "Epoch 6 Step: 3150 Val loss: 1.5140269994735718\n",
            "Epoch 6 Step: 3175 Val loss: 1.5347439050674438\n",
            "Epoch 6 Step: 3200 Val loss: 1.5230128765106201\n",
            "Epoch 6 Step: 3225 Val loss: 1.5084264278411865\n",
            "Epoch 6 Step: 3250 Val loss: 1.5172067880630493\n",
            "Epoch 6 Step: 3275 Val loss: 1.529354214668274\n",
            "Epoch 6 Step: 3300 Val loss: 1.5213172435760498\n",
            "Epoch 6 Step: 3325 Val loss: 1.5258450508117676\n",
            "Epoch 6 Step: 3350 Val loss: 1.519703984260559\n",
            "Epoch 6 Step: 3375 Val loss: 1.516907811164856\n",
            "Epoch 6 Step: 3400 Val loss: 1.5180071592330933\n",
            "Epoch 6 Step: 3425 Val loss: 1.510529637336731\n",
            "Epoch 7 Step: 3450 Val loss: 1.527011513710022\n",
            "Epoch 7 Step: 3475 Val loss: 1.5195423364639282\n",
            "Epoch 7 Step: 3500 Val loss: 1.508793592453003\n",
            "Epoch 7 Step: 3525 Val loss: 1.5067219734191895\n",
            "Epoch 7 Step: 3550 Val loss: 1.53408682346344\n",
            "Epoch 7 Step: 3575 Val loss: 1.5235509872436523\n",
            "Epoch 7 Step: 3600 Val loss: 1.510507583618164\n",
            "Epoch 7 Step: 3625 Val loss: 1.5300694704055786\n",
            "Epoch 7 Step: 3650 Val loss: 1.520782232284546\n",
            "Epoch 7 Step: 3675 Val loss: 1.497114658355713\n",
            "Epoch 7 Step: 3700 Val loss: 1.5044411420822144\n",
            "Epoch 7 Step: 3725 Val loss: 1.5100314617156982\n",
            "Epoch 7 Step: 3750 Val loss: 1.5255405902862549\n",
            "Epoch 7 Step: 3775 Val loss: 1.5161999464035034\n",
            "Epoch 7 Step: 3800 Val loss: 1.5062506198883057\n",
            "Epoch 7 Step: 3825 Val loss: 1.5212100744247437\n",
            "Epoch 7 Step: 3850 Val loss: 1.5116552114486694\n",
            "Epoch 7 Step: 3875 Val loss: 1.5198968648910522\n",
            "Epoch 7 Step: 3900 Val loss: 1.518568515777588\n",
            "Epoch 8 Step: 3925 Val loss: 1.5179378986358643\n",
            "Epoch 8 Step: 3950 Val loss: 1.5040212869644165\n",
            "Epoch 8 Step: 3975 Val loss: 1.5297110080718994\n",
            "Epoch 8 Step: 4000 Val loss: 1.5104398727416992\n",
            "Epoch 8 Step: 4025 Val loss: 1.5201410055160522\n",
            "Epoch 8 Step: 4050 Val loss: 1.5150424242019653\n",
            "Epoch 8 Step: 4075 Val loss: 1.531415343284607\n",
            "Epoch 8 Step: 4100 Val loss: 1.5271246433258057\n",
            "Epoch 8 Step: 4125 Val loss: 1.5003271102905273\n",
            "Epoch 8 Step: 4150 Val loss: 1.5218271017074585\n",
            "Epoch 8 Step: 4175 Val loss: 1.5035674571990967\n",
            "Epoch 8 Step: 4200 Val loss: 1.5191019773483276\n",
            "Epoch 8 Step: 4225 Val loss: 1.51412034034729\n",
            "Epoch 8 Step: 4250 Val loss: 1.512929081916809\n",
            "Epoch 8 Step: 4275 Val loss: 1.5216456651687622\n",
            "Epoch 8 Step: 4300 Val loss: 1.5136100053787231\n",
            "Epoch 8 Step: 4325 Val loss: 1.5300533771514893\n",
            "Epoch 8 Step: 4350 Val loss: 1.5167012214660645\n",
            "Epoch 8 Step: 4375 Val loss: 1.513046145439148\n",
            "Epoch 8 Step: 4400 Val loss: 1.5119528770446777\n",
            "Epoch 9 Step: 4425 Val loss: 1.4954229593276978\n",
            "Epoch 9 Step: 4450 Val loss: 1.5068747997283936\n",
            "Epoch 9 Step: 4475 Val loss: 1.5334091186523438\n",
            "Epoch 9 Step: 4500 Val loss: 1.5028588771820068\n",
            "Epoch 9 Step: 4525 Val loss: 1.4999537467956543\n",
            "Epoch 9 Step: 4550 Val loss: 1.5133687257766724\n",
            "Epoch 9 Step: 4575 Val loss: 1.5108354091644287\n",
            "Epoch 9 Step: 4600 Val loss: 1.5093437433242798\n",
            "Epoch 9 Step: 4625 Val loss: 1.5199435949325562\n",
            "Epoch 9 Step: 4650 Val loss: 1.5087412595748901\n",
            "Epoch 9 Step: 4675 Val loss: 1.49558687210083\n",
            "Epoch 9 Step: 4700 Val loss: 1.5111910104751587\n",
            "Epoch 9 Step: 4725 Val loss: 1.5093580484390259\n",
            "Epoch 9 Step: 4750 Val loss: 1.4951972961425781\n",
            "Epoch 9 Step: 4775 Val loss: 1.5187968015670776\n",
            "Epoch 9 Step: 4800 Val loss: 1.5183181762695312\n",
            "Epoch 9 Step: 4825 Val loss: 1.5131930112838745\n",
            "Epoch 9 Step: 4850 Val loss: 1.5250287055969238\n",
            "Epoch 9 Step: 4875 Val loss: 1.5136748552322388\n",
            "Epoch 9 Step: 4900 Val loss: 1.5084911584854126\n",
            "Epoch 10 Step: 4925 Val loss: 1.5265964269638062\n",
            "Epoch 10 Step: 4950 Val loss: 1.5173782110214233\n",
            "Epoch 10 Step: 4975 Val loss: 1.5257644653320312\n",
            "Epoch 10 Step: 5000 Val loss: 1.506753921508789\n",
            "Epoch 10 Step: 5025 Val loss: 1.5018333196640015\n",
            "Epoch 10 Step: 5050 Val loss: 1.4984688758850098\n",
            "Epoch 10 Step: 5075 Val loss: 1.5384045839309692\n",
            "Epoch 10 Step: 5100 Val loss: 1.5084989070892334\n",
            "Epoch 10 Step: 5125 Val loss: 1.5054737329483032\n",
            "Epoch 10 Step: 5150 Val loss: 1.5043877363204956\n",
            "Epoch 10 Step: 5175 Val loss: 1.5319634675979614\n",
            "Epoch 10 Step: 5200 Val loss: 1.5032635927200317\n",
            "Epoch 10 Step: 5225 Val loss: 1.5201364755630493\n",
            "Epoch 10 Step: 5250 Val loss: 1.5168235301971436\n",
            "Epoch 10 Step: 5275 Val loss: 1.5084911584854126\n",
            "Epoch 10 Step: 5300 Val loss: 1.506453514099121\n",
            "Epoch 10 Step: 5325 Val loss: 1.5182832479476929\n",
            "Epoch 10 Step: 5350 Val loss: 1.5218621492385864\n",
            "Epoch 10 Step: 5375 Val loss: 1.4994744062423706\n",
            "Epoch 11 Step: 5400 Val loss: 1.4935417175292969\n",
            "Epoch 11 Step: 5425 Val loss: 1.51008939743042\n",
            "Epoch 11 Step: 5450 Val loss: 1.5286526679992676\n",
            "Epoch 11 Step: 5475 Val loss: 1.5057233572006226\n",
            "Epoch 11 Step: 5500 Val loss: 1.5231560468673706\n",
            "Epoch 11 Step: 5525 Val loss: 1.518134355545044\n",
            "Epoch 11 Step: 5550 Val loss: 1.5053913593292236\n",
            "Epoch 11 Step: 5575 Val loss: 1.5151969194412231\n",
            "Epoch 11 Step: 5600 Val loss: 1.5118225812911987\n",
            "Epoch 11 Step: 5625 Val loss: 1.5358372926712036\n",
            "Epoch 11 Step: 5650 Val loss: 1.5125741958618164\n",
            "Epoch 11 Step: 5675 Val loss: 1.501664161682129\n",
            "Epoch 11 Step: 5700 Val loss: 1.5058190822601318\n",
            "Epoch 11 Step: 5725 Val loss: 1.5257703065872192\n",
            "Epoch 11 Step: 5750 Val loss: 1.52027428150177\n",
            "Epoch 11 Step: 5775 Val loss: 1.5169236660003662\n",
            "Epoch 11 Step: 5800 Val loss: 1.5231331586837769\n",
            "Epoch 11 Step: 5825 Val loss: 1.5104126930236816\n",
            "Epoch 11 Step: 5850 Val loss: 1.527265191078186\n",
            "Epoch 11 Step: 5875 Val loss: 1.5129754543304443\n",
            "Epoch 12 Step: 5900 Val loss: 1.5222612619400024\n",
            "Epoch 12 Step: 5925 Val loss: 1.5129451751708984\n",
            "Epoch 12 Step: 5950 Val loss: 1.5226571559906006\n",
            "Epoch 12 Step: 5975 Val loss: 1.5137499570846558\n",
            "Epoch 12 Step: 6000 Val loss: 1.5422097444534302\n",
            "Epoch 12 Step: 6025 Val loss: 1.5281150341033936\n",
            "Epoch 12 Step: 6050 Val loss: 1.5084962844848633\n",
            "Epoch 12 Step: 6075 Val loss: 1.529921054840088\n",
            "Epoch 12 Step: 6100 Val loss: 1.5255048274993896\n",
            "Epoch 12 Step: 6125 Val loss: 1.497023344039917\n",
            "Epoch 12 Step: 6150 Val loss: 1.4990683794021606\n",
            "Epoch 12 Step: 6175 Val loss: 1.5086711645126343\n",
            "Epoch 12 Step: 6200 Val loss: 1.5181043148040771\n",
            "Epoch 12 Step: 6225 Val loss: 1.5070480108261108\n",
            "Epoch 12 Step: 6250 Val loss: 1.5070688724517822\n",
            "Epoch 12 Step: 6275 Val loss: 1.518736720085144\n",
            "Epoch 12 Step: 6300 Val loss: 1.5049382448196411\n",
            "Epoch 12 Step: 6325 Val loss: 1.524229645729065\n",
            "Epoch 12 Step: 6350 Val loss: 1.5115675926208496\n",
            "Epoch 13 Step: 6375 Val loss: 1.5108634233474731\n",
            "Epoch 13 Step: 6400 Val loss: 1.4945390224456787\n",
            "Epoch 13 Step: 6425 Val loss: 1.540008783340454\n",
            "Epoch 13 Step: 6450 Val loss: 1.5132825374603271\n",
            "Epoch 13 Step: 6475 Val loss: 1.519308090209961\n",
            "Epoch 13 Step: 6500 Val loss: 1.5185548067092896\n",
            "Epoch 13 Step: 6525 Val loss: 1.5256078243255615\n",
            "Epoch 13 Step: 6550 Val loss: 1.5302621126174927\n",
            "Epoch 13 Step: 6575 Val loss: 1.5080448389053345\n",
            "Epoch 13 Step: 6600 Val loss: 1.5121971368789673\n",
            "Epoch 13 Step: 6625 Val loss: 1.507588267326355\n",
            "Epoch 13 Step: 6650 Val loss: 1.5271934270858765\n",
            "Epoch 13 Step: 6675 Val loss: 1.5215550661087036\n",
            "Epoch 13 Step: 6700 Val loss: 1.5170819759368896\n",
            "Epoch 13 Step: 6725 Val loss: 1.5283962488174438\n",
            "Epoch 13 Step: 6750 Val loss: 1.5148367881774902\n",
            "Epoch 13 Step: 6775 Val loss: 1.5360641479492188\n",
            "Epoch 13 Step: 6800 Val loss: 1.522311806678772\n",
            "Epoch 13 Step: 6825 Val loss: 1.51743745803833\n",
            "Epoch 13 Step: 6850 Val loss: 1.5209811925888062\n",
            "Epoch 14 Step: 6875 Val loss: 1.5043832063674927\n",
            "Epoch 14 Step: 6900 Val loss: 1.5106645822525024\n",
            "Epoch 14 Step: 6925 Val loss: 1.5419347286224365\n",
            "Epoch 14 Step: 6950 Val loss: 1.5091711282730103\n",
            "Epoch 14 Step: 6975 Val loss: 1.5151240825653076\n",
            "Epoch 14 Step: 7000 Val loss: 1.5179674625396729\n",
            "Epoch 14 Step: 7025 Val loss: 1.5265278816223145\n",
            "Epoch 14 Step: 7050 Val loss: 1.517147421836853\n",
            "Epoch 14 Step: 7075 Val loss: 1.530504584312439\n",
            "Epoch 14 Step: 7100 Val loss: 1.519579291343689\n",
            "Epoch 14 Step: 7125 Val loss: 1.5060774087905884\n",
            "Epoch 14 Step: 7150 Val loss: 1.5287213325500488\n",
            "Epoch 14 Step: 7175 Val loss: 1.509315848350525\n",
            "Epoch 14 Step: 7200 Val loss: 1.4970455169677734\n",
            "Epoch 14 Step: 7225 Val loss: 1.5201530456542969\n",
            "Epoch 14 Step: 7250 Val loss: 1.5276843309402466\n",
            "Epoch 14 Step: 7275 Val loss: 1.5235241651535034\n",
            "Epoch 14 Step: 7300 Val loss: 1.5385257005691528\n",
            "Epoch 14 Step: 7325 Val loss: 1.5123584270477295\n",
            "Epoch 14 Step: 7350 Val loss: 1.513169288635254\n",
            "Epoch 15 Step: 7375 Val loss: 1.5355117321014404\n",
            "Epoch 15 Step: 7400 Val loss: 1.529227375984192\n",
            "Epoch 15 Step: 7425 Val loss: 1.5385560989379883\n",
            "Epoch 15 Step: 7450 Val loss: 1.5087591409683228\n",
            "Epoch 15 Step: 7475 Val loss: 1.5097544193267822\n",
            "Epoch 15 Step: 7500 Val loss: 1.5062096118927002\n",
            "Epoch 15 Step: 7525 Val loss: 1.5388646125793457\n",
            "Epoch 15 Step: 7550 Val loss: 1.5157948732376099\n",
            "Epoch 15 Step: 7575 Val loss: 1.5118467807769775\n",
            "Epoch 15 Step: 7600 Val loss: 1.5162447690963745\n",
            "Epoch 15 Step: 7625 Val loss: 1.5407754182815552\n",
            "Epoch 15 Step: 7650 Val loss: 1.528948187828064\n",
            "Epoch 15 Step: 7675 Val loss: 1.535888433456421\n",
            "Epoch 15 Step: 7700 Val loss: 1.5315691232681274\n",
            "Epoch 15 Step: 7725 Val loss: 1.5196620225906372\n",
            "Epoch 15 Step: 7750 Val loss: 1.5075628757476807\n",
            "Epoch 15 Step: 7775 Val loss: 1.5241684913635254\n",
            "Epoch 15 Step: 7800 Val loss: 1.5379472970962524\n",
            "Epoch 15 Step: 7825 Val loss: 1.5083849430084229\n",
            "Epoch 16 Step: 7850 Val loss: 1.5053417682647705\n",
            "Epoch 16 Step: 7875 Val loss: 1.5117162466049194\n",
            "Epoch 16 Step: 7900 Val loss: 1.5492935180664062\n",
            "Epoch 16 Step: 7925 Val loss: 1.5228936672210693\n",
            "Epoch 16 Step: 7950 Val loss: 1.5234334468841553\n",
            "Epoch 16 Step: 7975 Val loss: 1.5236046314239502\n",
            "Epoch 16 Step: 8000 Val loss: 1.5072886943817139\n",
            "Epoch 16 Step: 8025 Val loss: 1.528888463973999\n",
            "Epoch 16 Step: 8050 Val loss: 1.5189940929412842\n",
            "Epoch 16 Step: 8075 Val loss: 1.5425244569778442\n",
            "Epoch 16 Step: 8100 Val loss: 1.517604112625122\n",
            "Epoch 16 Step: 8125 Val loss: 1.501899242401123\n",
            "Epoch 16 Step: 8150 Val loss: 1.5187753438949585\n",
            "Epoch 16 Step: 8175 Val loss: 1.5457558631896973\n",
            "Epoch 16 Step: 8200 Val loss: 1.531558871269226\n",
            "Epoch 16 Step: 8225 Val loss: 1.525266170501709\n",
            "Epoch 16 Step: 8250 Val loss: 1.5247772932052612\n",
            "Epoch 16 Step: 8275 Val loss: 1.535490870475769\n",
            "Epoch 16 Step: 8300 Val loss: 1.5346794128417969\n",
            "Epoch 16 Step: 8325 Val loss: 1.5166850090026855\n",
            "Epoch 17 Step: 8350 Val loss: 1.5361930131912231\n",
            "Epoch 17 Step: 8375 Val loss: 1.520050287246704\n",
            "Epoch 17 Step: 8400 Val loss: 1.5328760147094727\n",
            "Epoch 17 Step: 8425 Val loss: 1.5320273637771606\n",
            "Epoch 17 Step: 8450 Val loss: 1.552335262298584\n",
            "Epoch 17 Step: 8475 Val loss: 1.530653715133667\n",
            "Epoch 17 Step: 8500 Val loss: 1.5187580585479736\n",
            "Epoch 17 Step: 8525 Val loss: 1.5441120862960815\n",
            "Epoch 17 Step: 8550 Val loss: 1.529148817062378\n",
            "Epoch 17 Step: 8575 Val loss: 1.4909675121307373\n",
            "Epoch 17 Step: 8600 Val loss: 1.5014591217041016\n",
            "Epoch 17 Step: 8625 Val loss: 1.5290868282318115\n",
            "Epoch 17 Step: 8650 Val loss: 1.5165892839431763\n",
            "Epoch 17 Step: 8675 Val loss: 1.5163898468017578\n",
            "Epoch 17 Step: 8700 Val loss: 1.511723279953003\n",
            "Epoch 17 Step: 8725 Val loss: 1.530699610710144\n",
            "Epoch 17 Step: 8750 Val loss: 1.510447382926941\n",
            "Epoch 17 Step: 8775 Val loss: 1.5407248735427856\n",
            "Epoch 17 Step: 8800 Val loss: 1.5214141607284546\n",
            "Epoch 18 Step: 8825 Val loss: 1.5431766510009766\n",
            "Epoch 18 Step: 8850 Val loss: 1.5163252353668213\n",
            "Epoch 18 Step: 8875 Val loss: 1.5555742979049683\n",
            "Epoch 18 Step: 8900 Val loss: 1.5288161039352417\n",
            "Epoch 18 Step: 8925 Val loss: 1.5459840297698975\n",
            "Epoch 18 Step: 8950 Val loss: 1.5423957109451294\n",
            "Epoch 18 Step: 8975 Val loss: 1.54127037525177\n",
            "Epoch 18 Step: 9000 Val loss: 1.5417544841766357\n",
            "Epoch 18 Step: 9025 Val loss: 1.5189509391784668\n",
            "Epoch 18 Step: 9050 Val loss: 1.527323603630066\n",
            "Epoch 18 Step: 9075 Val loss: 1.509903073310852\n",
            "Epoch 18 Step: 9100 Val loss: 1.5422260761260986\n",
            "Epoch 18 Step: 9125 Val loss: 1.5499346256256104\n",
            "Epoch 18 Step: 9150 Val loss: 1.5311076641082764\n",
            "Epoch 18 Step: 9175 Val loss: 1.5346428155899048\n",
            "Epoch 18 Step: 9200 Val loss: 1.5302786827087402\n",
            "Epoch 18 Step: 9225 Val loss: 1.5460838079452515\n",
            "Epoch 18 Step: 9250 Val loss: 1.5378749370574951\n",
            "Epoch 18 Step: 9275 Val loss: 1.5225054025650024\n",
            "Epoch 18 Step: 9300 Val loss: 1.5356885194778442\n",
            "Epoch 19 Step: 9325 Val loss: 1.518144965171814\n",
            "Epoch 19 Step: 9350 Val loss: 1.519560694694519\n",
            "Epoch 19 Step: 9375 Val loss: 1.555951714515686\n",
            "Epoch 19 Step: 9400 Val loss: 1.5115270614624023\n",
            "Epoch 19 Step: 9425 Val loss: 1.5287683010101318\n",
            "Epoch 19 Step: 9450 Val loss: 1.5249419212341309\n",
            "Epoch 19 Step: 9475 Val loss: 1.5439536571502686\n",
            "Epoch 19 Step: 9500 Val loss: 1.5287621021270752\n",
            "Epoch 19 Step: 9525 Val loss: 1.5484418869018555\n",
            "Epoch 19 Step: 9550 Val loss: 1.5280593633651733\n",
            "Epoch 19 Step: 9575 Val loss: 1.5074926614761353\n",
            "Epoch 19 Step: 9600 Val loss: 1.531850814819336\n",
            "Epoch 19 Step: 9625 Val loss: 1.5349174737930298\n",
            "Epoch 19 Step: 9650 Val loss: 1.5029780864715576\n",
            "Epoch 19 Step: 9675 Val loss: 1.5342246294021606\n",
            "Epoch 19 Step: 9700 Val loss: 1.5423465967178345\n",
            "Epoch 19 Step: 9725 Val loss: 1.5410687923431396\n",
            "Epoch 19 Step: 9750 Val loss: 1.5537275075912476\n",
            "Epoch 19 Step: 9775 Val loss: 1.529085397720337\n",
            "Epoch 19 Step: 9800 Val loss: 1.5191097259521484\n",
            "Epoch 20 Step: 9825 Val loss: 1.5442343950271606\n",
            "Epoch 20 Step: 9850 Val loss: 1.546353816986084\n",
            "Epoch 20 Step: 9875 Val loss: 1.5566699504852295\n",
            "Epoch 20 Step: 9900 Val loss: 1.5207717418670654\n",
            "Epoch 20 Step: 9925 Val loss: 1.5295366048812866\n",
            "Epoch 20 Step: 9950 Val loss: 1.5162016153335571\n",
            "Epoch 20 Step: 9975 Val loss: 1.544295072555542\n",
            "Epoch 20 Step: 10000 Val loss: 1.5290733575820923\n",
            "Epoch 20 Step: 10025 Val loss: 1.523486614227295\n",
            "Epoch 20 Step: 10050 Val loss: 1.5300182104110718\n",
            "Epoch 20 Step: 10075 Val loss: 1.5564186573028564\n",
            "Epoch 20 Step: 10100 Val loss: 1.5333425998687744\n",
            "Epoch 20 Step: 10125 Val loss: 1.5511043071746826\n",
            "Epoch 20 Step: 10150 Val loss: 1.5464814901351929\n",
            "Epoch 20 Step: 10175 Val loss: 1.5226261615753174\n",
            "Epoch 20 Step: 10200 Val loss: 1.529785394668579\n",
            "Epoch 20 Step: 10225 Val loss: 1.5468854904174805\n",
            "Epoch 20 Step: 10250 Val loss: 1.5551587343215942\n",
            "Epoch 20 Step: 10275 Val loss: 1.515045404434204\n",
            "Epoch 21 Step: 10300 Val loss: 1.521622657775879\n",
            "Epoch 21 Step: 10325 Val loss: 1.5237919092178345\n",
            "Epoch 21 Step: 10350 Val loss: 1.5660526752471924\n",
            "Epoch 21 Step: 10375 Val loss: 1.5515013933181763\n",
            "Epoch 21 Step: 10400 Val loss: 1.5482507944107056\n",
            "Epoch 21 Step: 10425 Val loss: 1.5285471677780151\n",
            "Epoch 21 Step: 10450 Val loss: 1.5336437225341797\n",
            "Epoch 21 Step: 10475 Val loss: 1.5437252521514893\n",
            "Epoch 21 Step: 10500 Val loss: 1.5439821481704712\n",
            "Epoch 21 Step: 10525 Val loss: 1.5681966543197632\n",
            "Epoch 21 Step: 10550 Val loss: 1.5469809770584106\n",
            "Epoch 21 Step: 10575 Val loss: 1.5105507373809814\n",
            "Epoch 21 Step: 10600 Val loss: 1.5350195169448853\n",
            "Epoch 21 Step: 10625 Val loss: 1.5508216619491577\n",
            "Epoch 21 Step: 10650 Val loss: 1.5319358110427856\n",
            "Epoch 21 Step: 10675 Val loss: 1.5341564416885376\n",
            "Epoch 21 Step: 10700 Val loss: 1.5342479944229126\n",
            "Epoch 21 Step: 10725 Val loss: 1.533743977546692\n",
            "Epoch 21 Step: 10750 Val loss: 1.5510811805725098\n",
            "Epoch 21 Step: 10775 Val loss: 1.5252665281295776\n",
            "Epoch 22 Step: 10800 Val loss: 1.546042561531067\n",
            "Epoch 22 Step: 10825 Val loss: 1.5336089134216309\n",
            "Epoch 22 Step: 10850 Val loss: 1.5411744117736816\n",
            "Epoch 22 Step: 10875 Val loss: 1.5446101427078247\n",
            "Epoch 22 Step: 10900 Val loss: 1.5688732862472534\n",
            "Epoch 22 Step: 10925 Val loss: 1.5376338958740234\n",
            "Epoch 22 Step: 10950 Val loss: 1.5206109285354614\n",
            "Epoch 22 Step: 10975 Val loss: 1.5640473365783691\n",
            "Epoch 22 Step: 11000 Val loss: 1.5518637895584106\n",
            "Epoch 22 Step: 11025 Val loss: 1.5069091320037842\n",
            "Epoch 22 Step: 11050 Val loss: 1.5137686729431152\n",
            "Epoch 22 Step: 11075 Val loss: 1.5333806276321411\n",
            "Epoch 22 Step: 11100 Val loss: 1.5490634441375732\n",
            "Epoch 22 Step: 11125 Val loss: 1.524543285369873\n",
            "Epoch 22 Step: 11150 Val loss: 1.5243160724639893\n",
            "Epoch 22 Step: 11175 Val loss: 1.5408612489700317\n",
            "Epoch 22 Step: 11200 Val loss: 1.5241655111312866\n",
            "Epoch 22 Step: 11225 Val loss: 1.5449296236038208\n",
            "Epoch 22 Step: 11250 Val loss: 1.5253552198410034\n",
            "Epoch 23 Step: 11275 Val loss: 1.5295330286026\n",
            "Epoch 23 Step: 11300 Val loss: 1.5248522758483887\n",
            "Epoch 23 Step: 11325 Val loss: 1.5611704587936401\n",
            "Epoch 23 Step: 11350 Val loss: 1.53818941116333\n",
            "Epoch 23 Step: 11375 Val loss: 1.553378701210022\n",
            "Epoch 23 Step: 11400 Val loss: 1.5507923364639282\n",
            "Epoch 23 Step: 11425 Val loss: 1.5531426668167114\n",
            "Epoch 23 Step: 11450 Val loss: 1.5500354766845703\n",
            "Epoch 23 Step: 11475 Val loss: 1.5350273847579956\n",
            "Epoch 23 Step: 11500 Val loss: 1.5345731973648071\n",
            "Epoch 23 Step: 11525 Val loss: 1.519616723060608\n",
            "Epoch 23 Step: 11550 Val loss: 1.5543508529663086\n",
            "Epoch 23 Step: 11575 Val loss: 1.5556997060775757\n",
            "Epoch 23 Step: 11600 Val loss: 1.535504937171936\n",
            "Epoch 23 Step: 11625 Val loss: 1.5418837070465088\n",
            "Epoch 23 Step: 11650 Val loss: 1.5551739931106567\n",
            "Epoch 23 Step: 11675 Val loss: 1.5604747533798218\n",
            "Epoch 23 Step: 11700 Val loss: 1.5472040176391602\n",
            "Epoch 23 Step: 11725 Val loss: 1.5450531244277954\n",
            "Epoch 23 Step: 11750 Val loss: 1.553834319114685\n",
            "Epoch 24 Step: 11775 Val loss: 1.5277069807052612\n",
            "Epoch 24 Step: 11800 Val loss: 1.535872220993042\n",
            "Epoch 24 Step: 11825 Val loss: 1.5783900022506714\n",
            "Epoch 24 Step: 11850 Val loss: 1.5426585674285889\n",
            "Epoch 24 Step: 11875 Val loss: 1.5391845703125\n",
            "Epoch 24 Step: 11900 Val loss: 1.5366250276565552\n",
            "Epoch 24 Step: 11925 Val loss: 1.5492619276046753\n",
            "Epoch 24 Step: 11950 Val loss: 1.5567748546600342\n",
            "Epoch 24 Step: 11975 Val loss: 1.5617362260818481\n",
            "Epoch 24 Step: 12000 Val loss: 1.5369468927383423\n",
            "Epoch 24 Step: 12025 Val loss: 1.5298086404800415\n",
            "Epoch 24 Step: 12050 Val loss: 1.538328766822815\n",
            "Epoch 24 Step: 12075 Val loss: 1.541659951210022\n",
            "Epoch 24 Step: 12100 Val loss: 1.5073071718215942\n",
            "Epoch 24 Step: 12125 Val loss: 1.5367716550827026\n",
            "Epoch 24 Step: 12150 Val loss: 1.5528911352157593\n",
            "Epoch 24 Step: 12175 Val loss: 1.5445702075958252\n",
            "Epoch 24 Step: 12200 Val loss: 1.5685878992080688\n",
            "Epoch 24 Step: 12225 Val loss: 1.541731357574463\n",
            "Epoch 24 Step: 12250 Val loss: 1.5411574840545654\n",
            "Epoch 25 Step: 12275 Val loss: 1.562191367149353\n",
            "Epoch 25 Step: 12300 Val loss: 1.5591599941253662\n",
            "Epoch 25 Step: 12325 Val loss: 1.5764832496643066\n",
            "Epoch 25 Step: 12350 Val loss: 1.5383365154266357\n",
            "Epoch 25 Step: 12375 Val loss: 1.542877197265625\n",
            "Epoch 25 Step: 12400 Val loss: 1.5228608846664429\n",
            "Epoch 25 Step: 12425 Val loss: 1.5544945001602173\n",
            "Epoch 25 Step: 12450 Val loss: 1.5589919090270996\n",
            "Epoch 25 Step: 12475 Val loss: 1.5487101078033447\n",
            "Epoch 25 Step: 12500 Val loss: 1.5544201135635376\n",
            "Epoch 25 Step: 12525 Val loss: 1.5747143030166626\n",
            "Epoch 25 Step: 12550 Val loss: 1.5488975048065186\n",
            "Epoch 25 Step: 12575 Val loss: 1.5487141609191895\n",
            "Epoch 25 Step: 12600 Val loss: 1.5423665046691895\n",
            "Epoch 25 Step: 12625 Val loss: 1.5299464464187622\n",
            "Epoch 25 Step: 12650 Val loss: 1.5424445867538452\n",
            "Epoch 25 Step: 12675 Val loss: 1.5519336462020874\n",
            "Epoch 25 Step: 12700 Val loss: 1.5697156190872192\n",
            "Epoch 25 Step: 12725 Val loss: 1.5143452882766724\n",
            "Epoch 26 Step: 12750 Val loss: 1.5341969728469849\n",
            "Epoch 26 Step: 12775 Val loss: 1.533937931060791\n",
            "Epoch 26 Step: 12800 Val loss: 1.5849039554595947\n",
            "Epoch 26 Step: 12825 Val loss: 1.553612232208252\n",
            "Epoch 26 Step: 12850 Val loss: 1.550903081893921\n",
            "Epoch 26 Step: 12875 Val loss: 1.5520039796829224\n",
            "Epoch 26 Step: 12900 Val loss: 1.537307858467102\n",
            "Epoch 26 Step: 12925 Val loss: 1.559196949005127\n",
            "Epoch 26 Step: 12950 Val loss: 1.5498852729797363\n",
            "Epoch 26 Step: 12975 Val loss: 1.58516263961792\n",
            "Epoch 26 Step: 13000 Val loss: 1.5586577653884888\n",
            "Epoch 26 Step: 13025 Val loss: 1.5384531021118164\n",
            "Epoch 26 Step: 13050 Val loss: 1.5530633926391602\n",
            "Epoch 26 Step: 13075 Val loss: 1.5735284090042114\n",
            "Epoch 26 Step: 13100 Val loss: 1.5392115116119385\n",
            "Epoch 26 Step: 13125 Val loss: 1.5404152870178223\n",
            "Epoch 26 Step: 13150 Val loss: 1.534378170967102\n",
            "Epoch 26 Step: 13175 Val loss: 1.5326772928237915\n",
            "Epoch 26 Step: 13200 Val loss: 1.554256558418274\n",
            "Epoch 26 Step: 13225 Val loss: 1.528427243232727\n",
            "Epoch 27 Step: 13250 Val loss: 1.5503051280975342\n",
            "Epoch 27 Step: 13275 Val loss: 1.5343050956726074\n",
            "Epoch 27 Step: 13300 Val loss: 1.5349645614624023\n",
            "Epoch 27 Step: 13325 Val loss: 1.5382806062698364\n",
            "Epoch 27 Step: 13350 Val loss: 1.5678470134735107\n",
            "Epoch 27 Step: 13375 Val loss: 1.5459215641021729\n",
            "Epoch 27 Step: 13400 Val loss: 1.5266448259353638\n",
            "Epoch 27 Step: 13425 Val loss: 1.5750186443328857\n",
            "Epoch 27 Step: 13450 Val loss: 1.5643625259399414\n",
            "Epoch 27 Step: 13475 Val loss: 1.5172266960144043\n",
            "Epoch 27 Step: 13500 Val loss: 1.51461660861969\n",
            "Epoch 27 Step: 13525 Val loss: 1.537075161933899\n",
            "Epoch 27 Step: 13550 Val loss: 1.5503530502319336\n",
            "Epoch 27 Step: 13575 Val loss: 1.5489468574523926\n",
            "Epoch 27 Step: 13600 Val loss: 1.529435396194458\n",
            "Epoch 27 Step: 13625 Val loss: 1.5564863681793213\n",
            "Epoch 27 Step: 13650 Val loss: 1.5297114849090576\n",
            "Epoch 27 Step: 13675 Val loss: 1.5687905550003052\n",
            "Epoch 27 Step: 13700 Val loss: 1.5335733890533447\n",
            "Epoch 28 Step: 13725 Val loss: 1.5676558017730713\n",
            "Epoch 28 Step: 13750 Val loss: 1.5366028547286987\n",
            "Epoch 28 Step: 13775 Val loss: 1.5724598169326782\n",
            "Epoch 28 Step: 13800 Val loss: 1.5472711324691772\n",
            "Epoch 28 Step: 13825 Val loss: 1.5666311979293823\n",
            "Epoch 28 Step: 13850 Val loss: 1.5689220428466797\n",
            "Epoch 28 Step: 13875 Val loss: 1.5606555938720703\n",
            "Epoch 28 Step: 13900 Val loss: 1.5612784624099731\n",
            "Epoch 28 Step: 13925 Val loss: 1.5392557382583618\n",
            "Epoch 28 Step: 13950 Val loss: 1.5529329776763916\n",
            "Epoch 28 Step: 13975 Val loss: 1.5258508920669556\n",
            "Epoch 28 Step: 14000 Val loss: 1.5768128633499146\n",
            "Epoch 28 Step: 14025 Val loss: 1.5627025365829468\n",
            "Epoch 28 Step: 14050 Val loss: 1.557997703552246\n",
            "Epoch 28 Step: 14075 Val loss: 1.5444213151931763\n",
            "Epoch 28 Step: 14100 Val loss: 1.5509406328201294\n",
            "Epoch 28 Step: 14125 Val loss: 1.5588014125823975\n",
            "Epoch 28 Step: 14150 Val loss: 1.5570019483566284\n",
            "Epoch 28 Step: 14175 Val loss: 1.5536129474639893\n",
            "Epoch 28 Step: 14200 Val loss: 1.5573915243148804\n",
            "Epoch 29 Step: 14225 Val loss: 1.5372198820114136\n",
            "Epoch 29 Step: 14250 Val loss: 1.5369040966033936\n",
            "Epoch 29 Step: 14275 Val loss: 1.5852627754211426\n",
            "Epoch 29 Step: 14300 Val loss: 1.5335416793823242\n",
            "Epoch 29 Step: 14325 Val loss: 1.5425196886062622\n",
            "Epoch 29 Step: 14350 Val loss: 1.5503315925598145\n",
            "Epoch 29 Step: 14375 Val loss: 1.5551825761795044\n",
            "Epoch 29 Step: 14400 Val loss: 1.553260087966919\n",
            "Epoch 29 Step: 14425 Val loss: 1.5715447664260864\n",
            "Epoch 29 Step: 14450 Val loss: 1.5444159507751465\n",
            "Epoch 29 Step: 14475 Val loss: 1.5379493236541748\n",
            "Epoch 29 Step: 14500 Val loss: 1.5475884675979614\n",
            "Epoch 29 Step: 14525 Val loss: 1.5471869707107544\n",
            "Epoch 29 Step: 14550 Val loss: 1.5079851150512695\n",
            "Epoch 29 Step: 14575 Val loss: 1.5634371042251587\n",
            "Epoch 29 Step: 14600 Val loss: 1.5597295761108398\n",
            "Epoch 29 Step: 14625 Val loss: 1.5563437938690186\n",
            "Epoch 29 Step: 14650 Val loss: 1.5682326555252075\n",
            "Epoch 29 Step: 14675 Val loss: 1.5423630475997925\n",
            "Epoch 29 Step: 14700 Val loss: 1.5386567115783691\n",
            "Epoch 30 Step: 14725 Val loss: 1.5597635507583618\n",
            "Epoch 30 Step: 14750 Val loss: 1.5640581846237183\n",
            "Epoch 30 Step: 14775 Val loss: 1.5671882629394531\n",
            "Epoch 30 Step: 14800 Val loss: 1.522778034210205\n",
            "Epoch 30 Step: 14825 Val loss: 1.5378854274749756\n",
            "Epoch 30 Step: 14850 Val loss: 1.5237666368484497\n",
            "Epoch 30 Step: 14875 Val loss: 1.546069860458374\n",
            "Epoch 30 Step: 14900 Val loss: 1.5496399402618408\n",
            "Epoch 30 Step: 14925 Val loss: 1.5264523029327393\n",
            "Epoch 30 Step: 14950 Val loss: 1.550118327140808\n",
            "Epoch 30 Step: 14975 Val loss: 1.5873792171478271\n",
            "Epoch 30 Step: 15000 Val loss: 1.542197585105896\n",
            "Epoch 30 Step: 15025 Val loss: 1.5497782230377197\n",
            "Epoch 30 Step: 15050 Val loss: 1.5412309169769287\n",
            "Epoch 30 Step: 15075 Val loss: 1.533079743385315\n",
            "Epoch 30 Step: 15100 Val loss: 1.5179513692855835\n",
            "Epoch 30 Step: 15125 Val loss: 1.5317381620407104\n",
            "Epoch 30 Step: 15150 Val loss: 1.561132550239563\n",
            "Epoch 30 Step: 15175 Val loss: 1.523245096206665\n",
            "Epoch 31 Step: 15200 Val loss: 1.521772027015686\n",
            "Epoch 31 Step: 15225 Val loss: 1.545074224472046\n",
            "Epoch 31 Step: 15250 Val loss: 1.5674657821655273\n",
            "Epoch 31 Step: 15275 Val loss: 1.556492567062378\n",
            "Epoch 31 Step: 15300 Val loss: 1.5602197647094727\n",
            "Epoch 31 Step: 15325 Val loss: 1.537413239479065\n",
            "Epoch 31 Step: 15350 Val loss: 1.5337799787521362\n",
            "Epoch 31 Step: 15375 Val loss: 1.5593767166137695\n",
            "Epoch 31 Step: 15400 Val loss: 1.544356346130371\n",
            "Epoch 31 Step: 15425 Val loss: 1.590561866760254\n",
            "Epoch 31 Step: 15450 Val loss: 1.5555821657180786\n",
            "Epoch 31 Step: 15475 Val loss: 1.542366862297058\n",
            "Epoch 31 Step: 15500 Val loss: 1.5484106540679932\n",
            "Epoch 31 Step: 15525 Val loss: 1.5636905431747437\n",
            "Epoch 31 Step: 15550 Val loss: 1.544860601425171\n",
            "Epoch 31 Step: 15575 Val loss: 1.5520868301391602\n",
            "Epoch 31 Step: 15600 Val loss: 1.5444461107254028\n",
            "Epoch 31 Step: 15625 Val loss: 1.5427581071853638\n",
            "Epoch 31 Step: 15650 Val loss: 1.5606563091278076\n",
            "Epoch 31 Step: 15675 Val loss: 1.5410877466201782\n",
            "Epoch 32 Step: 15700 Val loss: 1.561552882194519\n",
            "Epoch 32 Step: 15725 Val loss: 1.5404796600341797\n",
            "Epoch 32 Step: 15750 Val loss: 1.5370396375656128\n",
            "Epoch 32 Step: 15775 Val loss: 1.5269190073013306\n",
            "Epoch 32 Step: 15800 Val loss: 1.5844242572784424\n",
            "Epoch 32 Step: 15825 Val loss: 1.541870355606079\n",
            "Epoch 32 Step: 15850 Val loss: 1.5253185033798218\n",
            "Epoch 32 Step: 15875 Val loss: 1.577888011932373\n",
            "Epoch 32 Step: 15900 Val loss: 1.5645149946212769\n",
            "Epoch 32 Step: 15925 Val loss: 1.531921625137329\n",
            "Epoch 32 Step: 15950 Val loss: 1.520342230796814\n",
            "Epoch 32 Step: 15975 Val loss: 1.5632086992263794\n",
            "Epoch 32 Step: 16000 Val loss: 1.5558111667633057\n",
            "Epoch 32 Step: 16025 Val loss: 1.5392513275146484\n",
            "Epoch 32 Step: 16050 Val loss: 1.5353835821151733\n",
            "Epoch 32 Step: 16075 Val loss: 1.5583986043930054\n",
            "Epoch 32 Step: 16100 Val loss: 1.5370560884475708\n",
            "Epoch 32 Step: 16125 Val loss: 1.5759879350662231\n",
            "Epoch 32 Step: 16150 Val loss: 1.541709303855896\n",
            "Epoch 33 Step: 16175 Val loss: 1.5609047412872314\n",
            "Epoch 33 Step: 16200 Val loss: 1.5400145053863525\n",
            "Epoch 33 Step: 16225 Val loss: 1.5771551132202148\n",
            "Epoch 33 Step: 16250 Val loss: 1.5581289529800415\n",
            "Epoch 33 Step: 16275 Val loss: 1.578860878944397\n",
            "Epoch 33 Step: 16300 Val loss: 1.5745166540145874\n",
            "Epoch 33 Step: 16325 Val loss: 1.5720106363296509\n",
            "Epoch 33 Step: 16350 Val loss: 1.5641989707946777\n",
            "Epoch 33 Step: 16375 Val loss: 1.5360162258148193\n",
            "Epoch 33 Step: 16400 Val loss: 1.5586210489273071\n",
            "Epoch 33 Step: 16425 Val loss: 1.5359631776809692\n",
            "Epoch 33 Step: 16450 Val loss: 1.5721184015274048\n",
            "Epoch 33 Step: 16475 Val loss: 1.5858879089355469\n",
            "Epoch 33 Step: 16500 Val loss: 1.5703397989273071\n",
            "Epoch 33 Step: 16525 Val loss: 1.5573015213012695\n",
            "Epoch 33 Step: 16550 Val loss: 1.5569584369659424\n",
            "Epoch 33 Step: 16575 Val loss: 1.5596903562545776\n",
            "Epoch 33 Step: 16600 Val loss: 1.5570001602172852\n",
            "Epoch 33 Step: 16625 Val loss: 1.5569170713424683\n",
            "Epoch 33 Step: 16650 Val loss: 1.559450626373291\n",
            "Epoch 34 Step: 16675 Val loss: 1.5398375988006592\n",
            "Epoch 34 Step: 16700 Val loss: 1.5367995500564575\n",
            "Epoch 34 Step: 16725 Val loss: 1.5822798013687134\n",
            "Epoch 34 Step: 16750 Val loss: 1.5327967405319214\n",
            "Epoch 34 Step: 16775 Val loss: 1.540492296218872\n",
            "Epoch 34 Step: 16800 Val loss: 1.5460237264633179\n",
            "Epoch 34 Step: 16825 Val loss: 1.5563892126083374\n",
            "Epoch 34 Step: 16850 Val loss: 1.5466538667678833\n",
            "Epoch 34 Step: 16875 Val loss: 1.5711698532104492\n",
            "Epoch 34 Step: 16900 Val loss: 1.5586942434310913\n",
            "Epoch 34 Step: 16925 Val loss: 1.5366188287734985\n",
            "Epoch 34 Step: 16950 Val loss: 1.5736130475997925\n",
            "Epoch 34 Step: 16975 Val loss: 1.5526697635650635\n",
            "Epoch 34 Step: 17000 Val loss: 1.522825002670288\n",
            "Epoch 34 Step: 17025 Val loss: 1.5669562816619873\n",
            "Epoch 34 Step: 17050 Val loss: 1.5680766105651855\n",
            "Epoch 34 Step: 17075 Val loss: 1.5589122772216797\n",
            "Epoch 34 Step: 17100 Val loss: 1.571616530418396\n",
            "Epoch 34 Step: 17125 Val loss: 1.5528223514556885\n",
            "Epoch 34 Step: 17150 Val loss: 1.5407934188842773\n",
            "Epoch 35 Step: 17175 Val loss: 1.5731240510940552\n",
            "Epoch 35 Step: 17200 Val loss: 1.5670971870422363\n",
            "Epoch 35 Step: 17225 Val loss: 1.5786179304122925\n",
            "Epoch 35 Step: 17250 Val loss: 1.5355876684188843\n",
            "Epoch 35 Step: 17275 Val loss: 1.550514578819275\n",
            "Epoch 35 Step: 17300 Val loss: 1.5349352359771729\n",
            "Epoch 35 Step: 17325 Val loss: 1.556559681892395\n",
            "Epoch 35 Step: 17350 Val loss: 1.5672364234924316\n",
            "Epoch 35 Step: 17375 Val loss: 1.546019434928894\n",
            "Epoch 35 Step: 17400 Val loss: 1.5541291236877441\n",
            "Epoch 35 Step: 17425 Val loss: 1.590279459953308\n",
            "Epoch 35 Step: 17450 Val loss: 1.5538617372512817\n",
            "Epoch 35 Step: 17475 Val loss: 1.5799049139022827\n",
            "Epoch 35 Step: 17500 Val loss: 1.5632580518722534\n",
            "Epoch 35 Step: 17525 Val loss: 1.5405254364013672\n",
            "Epoch 35 Step: 17550 Val loss: 1.5423895120620728\n",
            "Epoch 35 Step: 17575 Val loss: 1.5608868598937988\n",
            "Epoch 35 Step: 17600 Val loss: 1.5800637006759644\n",
            "Epoch 35 Step: 17625 Val loss: 1.530356764793396\n",
            "Epoch 36 Step: 17650 Val loss: 1.5325217247009277\n",
            "Epoch 36 Step: 17675 Val loss: 1.5586707592010498\n",
            "Epoch 36 Step: 17700 Val loss: 1.5898683071136475\n",
            "Epoch 36 Step: 17725 Val loss: 1.5798768997192383\n",
            "Epoch 36 Step: 17750 Val loss: 1.5692030191421509\n",
            "Epoch 36 Step: 17775 Val loss: 1.5657484531402588\n",
            "Epoch 36 Step: 17800 Val loss: 1.542665958404541\n",
            "Epoch 36 Step: 17825 Val loss: 1.5702601671218872\n",
            "Epoch 36 Step: 17850 Val loss: 1.550017237663269\n",
            "Epoch 36 Step: 17875 Val loss: 1.5902290344238281\n",
            "Epoch 36 Step: 17900 Val loss: 1.5644711256027222\n",
            "Epoch 36 Step: 17925 Val loss: 1.539170742034912\n",
            "Epoch 36 Step: 17950 Val loss: 1.5581203699111938\n",
            "Epoch 36 Step: 17975 Val loss: 1.579833984375\n",
            "Epoch 36 Step: 18000 Val loss: 1.5603855848312378\n",
            "Epoch 36 Step: 18025 Val loss: 1.5680662393569946\n",
            "Epoch 36 Step: 18050 Val loss: 1.550423264503479\n",
            "Epoch 36 Step: 18075 Val loss: 1.5540730953216553\n",
            "Epoch 36 Step: 18100 Val loss: 1.5648695230484009\n",
            "Epoch 36 Step: 18125 Val loss: 1.5447686910629272\n",
            "Epoch 37 Step: 18150 Val loss: 1.5617910623550415\n",
            "Epoch 37 Step: 18175 Val loss: 1.5406460762023926\n",
            "Epoch 37 Step: 18200 Val loss: 1.553439736366272\n",
            "Epoch 37 Step: 18225 Val loss: 1.5570017099380493\n",
            "Epoch 37 Step: 18250 Val loss: 1.5878649950027466\n",
            "Epoch 37 Step: 18275 Val loss: 1.549256682395935\n",
            "Epoch 37 Step: 18300 Val loss: 1.5372166633605957\n",
            "Epoch 37 Step: 18325 Val loss: 1.5849767923355103\n",
            "Epoch 37 Step: 18350 Val loss: 1.5800535678863525\n",
            "Epoch 37 Step: 18375 Val loss: 1.52651047706604\n",
            "Epoch 37 Step: 18400 Val loss: 1.5299181938171387\n",
            "Epoch 37 Step: 18425 Val loss: 1.5577595233917236\n",
            "Epoch 37 Step: 18450 Val loss: 1.5711675882339478\n",
            "Epoch 37 Step: 18475 Val loss: 1.5567795038223267\n",
            "Epoch 37 Step: 18500 Val loss: 1.5367591381072998\n",
            "Epoch 37 Step: 18525 Val loss: 1.571132779121399\n",
            "Epoch 37 Step: 18550 Val loss: 1.540849208831787\n",
            "Epoch 37 Step: 18575 Val loss: 1.5723919868469238\n",
            "Epoch 37 Step: 18600 Val loss: 1.5567528009414673\n",
            "Epoch 38 Step: 18625 Val loss: 1.5744576454162598\n",
            "Epoch 38 Step: 18650 Val loss: 1.5509319305419922\n",
            "Epoch 38 Step: 18675 Val loss: 1.5902037620544434\n",
            "Epoch 38 Step: 18700 Val loss: 1.5463398694992065\n",
            "Epoch 38 Step: 18725 Val loss: 1.5673248767852783\n",
            "Epoch 38 Step: 18750 Val loss: 1.5723360776901245\n",
            "Epoch 38 Step: 18775 Val loss: 1.5571117401123047\n",
            "Epoch 38 Step: 18800 Val loss: 1.5713697671890259\n",
            "Epoch 38 Step: 18825 Val loss: 1.5323625802993774\n",
            "Epoch 38 Step: 18850 Val loss: 1.5626904964447021\n",
            "Epoch 38 Step: 18875 Val loss: 1.5348246097564697\n",
            "Epoch 38 Step: 18900 Val loss: 1.575790286064148\n",
            "Epoch 38 Step: 18925 Val loss: 1.5864605903625488\n",
            "Epoch 38 Step: 18950 Val loss: 1.5763179063796997\n",
            "Epoch 38 Step: 18975 Val loss: 1.550764560699463\n",
            "Epoch 38 Step: 19000 Val loss: 1.556469202041626\n",
            "Epoch 38 Step: 19025 Val loss: 1.5679923295974731\n",
            "Epoch 38 Step: 19050 Val loss: 1.5528349876403809\n",
            "Epoch 38 Step: 19075 Val loss: 1.5491713285446167\n",
            "Epoch 38 Step: 19100 Val loss: 1.5616270303726196\n",
            "Epoch 39 Step: 19125 Val loss: 1.5322380065917969\n",
            "Epoch 39 Step: 19150 Val loss: 1.5568504333496094\n",
            "Epoch 39 Step: 19175 Val loss: 1.5884798765182495\n",
            "Epoch 39 Step: 19200 Val loss: 1.5440824031829834\n",
            "Epoch 39 Step: 19225 Val loss: 1.5404900312423706\n",
            "Epoch 39 Step: 19250 Val loss: 1.5607068538665771\n",
            "Epoch 39 Step: 19275 Val loss: 1.574966549873352\n",
            "Epoch 39 Step: 19300 Val loss: 1.563245177268982\n",
            "Epoch 39 Step: 19325 Val loss: 1.5766690969467163\n",
            "Epoch 39 Step: 19350 Val loss: 1.556677222251892\n",
            "Epoch 39 Step: 19375 Val loss: 1.5597574710845947\n",
            "Epoch 39 Step: 19400 Val loss: 1.5708898305892944\n",
            "Epoch 39 Step: 19425 Val loss: 1.5708221197128296\n",
            "Epoch 39 Step: 19450 Val loss: 1.5208861827850342\n",
            "Epoch 39 Step: 19475 Val loss: 1.5670245885849\n",
            "Epoch 39 Step: 19500 Val loss: 1.5581156015396118\n",
            "Epoch 39 Step: 19525 Val loss: 1.5666651725769043\n",
            "Epoch 39 Step: 19550 Val loss: 1.5732040405273438\n",
            "Epoch 39 Step: 19575 Val loss: 1.5416278839111328\n",
            "Epoch 39 Step: 19600 Val loss: 1.535468339920044\n",
            "Epoch 40 Step: 19625 Val loss: 1.5707587003707886\n",
            "Epoch 40 Step: 19650 Val loss: 1.580849289894104\n",
            "Epoch 40 Step: 19675 Val loss: 1.6014692783355713\n",
            "Epoch 40 Step: 19700 Val loss: 1.542803168296814\n",
            "Epoch 40 Step: 19725 Val loss: 1.5599561929702759\n",
            "Epoch 40 Step: 19750 Val loss: 1.544372320175171\n",
            "Epoch 40 Step: 19775 Val loss: 1.5692099332809448\n",
            "Epoch 40 Step: 19800 Val loss: 1.5673636198043823\n",
            "Epoch 40 Step: 19825 Val loss: 1.5574252605438232\n",
            "Epoch 40 Step: 19850 Val loss: 1.5541514158248901\n",
            "Epoch 40 Step: 19875 Val loss: 1.596358060836792\n",
            "Epoch 40 Step: 19900 Val loss: 1.5660468339920044\n",
            "Epoch 40 Step: 19925 Val loss: 1.5781587362289429\n",
            "Epoch 40 Step: 19950 Val loss: 1.5623337030410767\n",
            "Epoch 40 Step: 19975 Val loss: 1.5571479797363281\n",
            "Epoch 40 Step: 20000 Val loss: 1.5545600652694702\n",
            "Epoch 40 Step: 20025 Val loss: 1.5506691932678223\n",
            "Epoch 40 Step: 20050 Val loss: 1.5861878395080566\n",
            "Epoch 40 Step: 20075 Val loss: 1.5354994535446167\n",
            "Epoch 41 Step: 20100 Val loss: 1.540939450263977\n",
            "Epoch 41 Step: 20125 Val loss: 1.564635157585144\n",
            "Epoch 41 Step: 20150 Val loss: 1.5776218175888062\n",
            "Epoch 41 Step: 20175 Val loss: 1.5659337043762207\n",
            "Epoch 41 Step: 20200 Val loss: 1.5680798292160034\n",
            "Epoch 41 Step: 20225 Val loss: 1.5667206048965454\n",
            "Epoch 41 Step: 20250 Val loss: 1.5484521389007568\n",
            "Epoch 41 Step: 20275 Val loss: 1.5816857814788818\n",
            "Epoch 41 Step: 20300 Val loss: 1.5653586387634277\n",
            "Epoch 41 Step: 20325 Val loss: 1.5943125486373901\n",
            "Epoch 41 Step: 20350 Val loss: 1.57282555103302\n",
            "Epoch 41 Step: 20375 Val loss: 1.5503222942352295\n",
            "Epoch 41 Step: 20400 Val loss: 1.5749096870422363\n",
            "Epoch 41 Step: 20425 Val loss: 1.6053111553192139\n",
            "Epoch 41 Step: 20450 Val loss: 1.5709021091461182\n",
            "Epoch 41 Step: 20475 Val loss: 1.5607688426971436\n",
            "Epoch 41 Step: 20500 Val loss: 1.54990553855896\n",
            "Epoch 41 Step: 20525 Val loss: 1.5542032718658447\n",
            "Epoch 41 Step: 20550 Val loss: 1.5740817785263062\n",
            "Epoch 41 Step: 20575 Val loss: 1.544105887413025\n",
            "Epoch 42 Step: 20600 Val loss: 1.5619868040084839\n",
            "Epoch 42 Step: 20625 Val loss: 1.5509721040725708\n",
            "Epoch 42 Step: 20650 Val loss: 1.5613908767700195\n",
            "Epoch 42 Step: 20675 Val loss: 1.5666028261184692\n",
            "Epoch 42 Step: 20700 Val loss: 1.610460877418518\n",
            "Epoch 42 Step: 20725 Val loss: 1.5629926919937134\n",
            "Epoch 42 Step: 20750 Val loss: 1.5433214902877808\n",
            "Epoch 42 Step: 20775 Val loss: 1.5846055746078491\n",
            "Epoch 42 Step: 20800 Val loss: 1.586376667022705\n",
            "Epoch 42 Step: 20825 Val loss: 1.5442934036254883\n",
            "Epoch 42 Step: 20850 Val loss: 1.5469638109207153\n",
            "Epoch 42 Step: 20875 Val loss: 1.5676989555358887\n",
            "Epoch 42 Step: 20900 Val loss: 1.5760517120361328\n",
            "Epoch 42 Step: 20925 Val loss: 1.5726066827774048\n",
            "Epoch 42 Step: 20950 Val loss: 1.544676423072815\n",
            "Epoch 42 Step: 20975 Val loss: 1.5798523426055908\n",
            "Epoch 42 Step: 21000 Val loss: 1.5453628301620483\n",
            "Epoch 42 Step: 21025 Val loss: 1.610801339149475\n",
            "Epoch 42 Step: 21050 Val loss: 1.5589268207550049\n",
            "Epoch 43 Step: 21075 Val loss: 1.582869291305542\n",
            "Epoch 43 Step: 21100 Val loss: 1.5531456470489502\n",
            "Epoch 43 Step: 21125 Val loss: 1.6045522689819336\n",
            "Epoch 43 Step: 21150 Val loss: 1.568050742149353\n",
            "Epoch 43 Step: 21175 Val loss: 1.5821664333343506\n",
            "Epoch 43 Step: 21200 Val loss: 1.5747559070587158\n",
            "Epoch 43 Step: 21225 Val loss: 1.5725654363632202\n",
            "Epoch 43 Step: 21250 Val loss: 1.573642373085022\n",
            "Epoch 43 Step: 21275 Val loss: 1.6068744659423828\n",
            "Epoch 43 Step: 21300 Val loss: 1.5682368278503418\n",
            "Epoch 43 Step: 21325 Val loss: 1.547255277633667\n",
            "Epoch 43 Step: 21350 Val loss: 1.5755354166030884\n",
            "Epoch 43 Step: 21375 Val loss: 1.5950881242752075\n",
            "Epoch 43 Step: 21400 Val loss: 1.5956904888153076\n",
            "Epoch 43 Step: 21425 Val loss: 1.581193447113037\n",
            "Epoch 43 Step: 21450 Val loss: 1.584213137626648\n",
            "Epoch 43 Step: 21475 Val loss: 1.581241488456726\n",
            "Epoch 43 Step: 21500 Val loss: 1.5750253200531006\n",
            "Epoch 43 Step: 21525 Val loss: 1.5729901790618896\n",
            "Epoch 43 Step: 21550 Val loss: 1.5716923475265503\n",
            "Epoch 44 Step: 21575 Val loss: 1.5409499406814575\n",
            "Epoch 44 Step: 21600 Val loss: 1.5463614463806152\n",
            "Epoch 44 Step: 21625 Val loss: 1.6077024936676025\n",
            "Epoch 44 Step: 21650 Val loss: 1.5567327737808228\n",
            "Epoch 44 Step: 21675 Val loss: 1.5594143867492676\n",
            "Epoch 44 Step: 21700 Val loss: 1.5686943531036377\n",
            "Epoch 44 Step: 21725 Val loss: 1.5759944915771484\n",
            "Epoch 44 Step: 21750 Val loss: 1.569307804107666\n",
            "Epoch 44 Step: 21775 Val loss: 1.5848308801651\n",
            "Epoch 44 Step: 21800 Val loss: 1.5662964582443237\n",
            "Epoch 44 Step: 21825 Val loss: 1.5570156574249268\n",
            "Epoch 44 Step: 21850 Val loss: 1.587048888206482\n",
            "Epoch 44 Step: 21875 Val loss: 1.5826250314712524\n",
            "Epoch 44 Step: 21900 Val loss: 1.5261473655700684\n",
            "Epoch 44 Step: 21925 Val loss: 1.5793733596801758\n",
            "Epoch 44 Step: 21950 Val loss: 1.583115577697754\n",
            "Epoch 44 Step: 21975 Val loss: 1.5704253911972046\n",
            "Epoch 44 Step: 22000 Val loss: 1.589200735092163\n",
            "Epoch 44 Step: 22025 Val loss: 1.5529788732528687\n",
            "Epoch 44 Step: 22050 Val loss: 1.550482988357544\n",
            "Epoch 45 Step: 22075 Val loss: 1.5861995220184326\n",
            "Epoch 45 Step: 22100 Val loss: 1.5790793895721436\n",
            "Epoch 45 Step: 22125 Val loss: 1.6104575395584106\n",
            "Epoch 45 Step: 22150 Val loss: 1.5373646020889282\n",
            "Epoch 45 Step: 22175 Val loss: 1.575136423110962\n",
            "Epoch 45 Step: 22200 Val loss: 1.5431249141693115\n",
            "Epoch 45 Step: 22225 Val loss: 1.574798583984375\n",
            "Epoch 45 Step: 22250 Val loss: 1.5889679193496704\n",
            "Epoch 45 Step: 22275 Val loss: 1.5747625827789307\n",
            "Epoch 45 Step: 22300 Val loss: 1.5716274976730347\n",
            "Epoch 45 Step: 22325 Val loss: 1.581206202507019\n",
            "Epoch 45 Step: 22350 Val loss: 1.579164981842041\n",
            "Epoch 45 Step: 22375 Val loss: 1.5882506370544434\n",
            "Epoch 45 Step: 22400 Val loss: 1.5736901760101318\n",
            "Epoch 45 Step: 22425 Val loss: 1.5619803667068481\n",
            "Epoch 45 Step: 22450 Val loss: 1.5817131996154785\n",
            "Epoch 45 Step: 22475 Val loss: 1.5110139846801758\n",
            "Epoch 45 Step: 22500 Val loss: 1.545886754989624\n",
            "Epoch 45 Step: 22525 Val loss: 1.5325144529342651\n",
            "Epoch 46 Step: 22550 Val loss: 1.5266258716583252\n",
            "Epoch 46 Step: 22575 Val loss: 1.5496083498001099\n",
            "Epoch 46 Step: 22600 Val loss: 1.5512179136276245\n",
            "Epoch 46 Step: 22625 Val loss: 1.5401017665863037\n",
            "Epoch 46 Step: 22650 Val loss: 1.5481290817260742\n",
            "Epoch 46 Step: 22675 Val loss: 1.5353484153747559\n",
            "Epoch 46 Step: 22700 Val loss: 1.5386593341827393\n",
            "Epoch 46 Step: 22725 Val loss: 1.5538151264190674\n",
            "Epoch 46 Step: 22750 Val loss: 1.5415763854980469\n",
            "Epoch 46 Step: 22775 Val loss: 1.5655800104141235\n",
            "Epoch 46 Step: 22800 Val loss: 1.5479105710983276\n",
            "Epoch 46 Step: 22825 Val loss: 1.5358456373214722\n",
            "Epoch 46 Step: 22850 Val loss: 1.545058250427246\n",
            "Epoch 46 Step: 22875 Val loss: 1.5676661729812622\n",
            "Epoch 46 Step: 22900 Val loss: 1.5488985776901245\n",
            "Epoch 46 Step: 22925 Val loss: 1.5512752532958984\n",
            "Epoch 46 Step: 22950 Val loss: 1.5461552143096924\n",
            "Epoch 46 Step: 22975 Val loss: 1.5568677186965942\n",
            "Epoch 46 Step: 23000 Val loss: 1.5550092458724976\n",
            "Epoch 46 Step: 23025 Val loss: 1.553360104560852\n",
            "Epoch 47 Step: 23050 Val loss: 1.558383584022522\n",
            "Epoch 47 Step: 23075 Val loss: 1.5438001155853271\n",
            "Epoch 47 Step: 23100 Val loss: 1.5352120399475098\n",
            "Epoch 47 Step: 23125 Val loss: 1.5310639142990112\n",
            "Epoch 47 Step: 23150 Val loss: 1.5775662660598755\n",
            "Epoch 47 Step: 23175 Val loss: 1.5571818351745605\n",
            "Epoch 47 Step: 23200 Val loss: 1.5370155572891235\n",
            "Epoch 47 Step: 23225 Val loss: 1.5688152313232422\n",
            "Epoch 47 Step: 23250 Val loss: 1.5774701833724976\n",
            "Epoch 47 Step: 23275 Val loss: 1.5451890230178833\n",
            "Epoch 47 Step: 23300 Val loss: 1.5407408475875854\n",
            "Epoch 47 Step: 23325 Val loss: 1.5705208778381348\n",
            "Epoch 47 Step: 23350 Val loss: 1.5633656978607178\n",
            "Epoch 47 Step: 23375 Val loss: 1.5515183210372925\n",
            "Epoch 47 Step: 23400 Val loss: 1.5408954620361328\n",
            "Epoch 47 Step: 23425 Val loss: 1.5655245780944824\n",
            "Epoch 47 Step: 23450 Val loss: 1.5363653898239136\n",
            "Epoch 47 Step: 23475 Val loss: 1.5671864748001099\n",
            "Epoch 47 Step: 23500 Val loss: 1.552338719367981\n",
            "Epoch 48 Step: 23525 Val loss: 1.5704951286315918\n",
            "Epoch 48 Step: 23550 Val loss: 1.5414150953292847\n",
            "Epoch 48 Step: 23575 Val loss: 1.5867409706115723\n",
            "Epoch 48 Step: 23600 Val loss: 1.5515637397766113\n",
            "Epoch 48 Step: 23625 Val loss: 1.5707935094833374\n",
            "Epoch 48 Step: 23650 Val loss: 1.5667554140090942\n",
            "Epoch 48 Step: 23675 Val loss: 1.570949673652649\n",
            "Epoch 48 Step: 23700 Val loss: 1.5726380348205566\n",
            "Epoch 48 Step: 23725 Val loss: 1.563267707824707\n",
            "Epoch 48 Step: 23750 Val loss: 1.566052794456482\n",
            "Epoch 48 Step: 23775 Val loss: 1.5561490058898926\n",
            "Epoch 48 Step: 23800 Val loss: 1.5737738609313965\n",
            "Epoch 48 Step: 23825 Val loss: 1.581046462059021\n",
            "Epoch 48 Step: 23850 Val loss: 1.5819673538208008\n",
            "Epoch 48 Step: 23875 Val loss: 1.5632885694503784\n",
            "Epoch 48 Step: 23900 Val loss: 1.5719573497772217\n",
            "Epoch 48 Step: 23925 Val loss: 1.5753225088119507\n",
            "Epoch 48 Step: 23950 Val loss: 1.57493257522583\n",
            "Epoch 48 Step: 23975 Val loss: 1.5627648830413818\n",
            "Epoch 48 Step: 24000 Val loss: 1.567615032196045\n",
            "Epoch 49 Step: 24025 Val loss: 1.5507276058197021\n",
            "Epoch 49 Step: 24050 Val loss: 1.548104166984558\n",
            "Epoch 49 Step: 24075 Val loss: 1.5865795612335205\n",
            "Epoch 49 Step: 24100 Val loss: 1.5656870603561401\n",
            "Epoch 49 Step: 24125 Val loss: 1.5570688247680664\n",
            "Epoch 49 Step: 24150 Val loss: 1.5686869621276855\n",
            "Epoch 49 Step: 24175 Val loss: 1.5733730792999268\n",
            "Epoch 49 Step: 24200 Val loss: 1.551543951034546\n",
            "Epoch 49 Step: 24225 Val loss: 1.58656644821167\n",
            "Epoch 49 Step: 24250 Val loss: 1.5822391510009766\n",
            "Epoch 49 Step: 24275 Val loss: 1.5605438947677612\n",
            "Epoch 49 Step: 24300 Val loss: 1.5858328342437744\n",
            "Epoch 49 Step: 24325 Val loss: 1.5732351541519165\n",
            "Epoch 49 Step: 24350 Val loss: 1.5368865728378296\n",
            "Epoch 49 Step: 24375 Val loss: 1.554861307144165\n",
            "Epoch 49 Step: 24400 Val loss: 1.5746630430221558\n",
            "Epoch 49 Step: 24425 Val loss: 1.5684776306152344\n",
            "Epoch 49 Step: 24450 Val loss: 1.57169771194458\n",
            "Epoch 49 Step: 24475 Val loss: 1.5509556531906128\n",
            "Epoch 49 Step: 24500 Val loss: 1.547523856163025\n",
            "Epoch 50 Step: 24525 Val loss: 1.5835034847259521\n",
            "Epoch 50 Step: 24550 Val loss: 1.5748159885406494\n",
            "Epoch 50 Step: 24575 Val loss: 1.5998053550720215\n",
            "Epoch 50 Step: 24600 Val loss: 1.5525500774383545\n",
            "Epoch 50 Step: 24625 Val loss: 1.5592745542526245\n",
            "Epoch 50 Step: 24650 Val loss: 1.5357519388198853\n",
            "Epoch 50 Step: 24675 Val loss: 1.5591464042663574\n",
            "Epoch 50 Step: 24700 Val loss: 1.572991132736206\n",
            "Epoch 50 Step: 24725 Val loss: 1.5544928312301636\n",
            "Epoch 50 Step: 24750 Val loss: 1.5573301315307617\n",
            "Epoch 50 Step: 24775 Val loss: 1.5837069749832153\n",
            "Epoch 50 Step: 24800 Val loss: 1.5683858394622803\n",
            "Epoch 50 Step: 24825 Val loss: 1.583999752998352\n",
            "Epoch 50 Step: 24850 Val loss: 1.5613430738449097\n",
            "Epoch 50 Step: 24875 Val loss: 1.5566123723983765\n",
            "Epoch 50 Step: 24900 Val loss: 1.551940679550171\n",
            "Epoch 50 Step: 24925 Val loss: 1.557674765586853\n",
            "Epoch 50 Step: 24950 Val loss: 1.5961309671401978\n",
            "Epoch 50 Step: 24975 Val loss: 1.5588586330413818\n",
            "Epoch 51 Step: 25000 Val loss: 1.5349998474121094\n",
            "Epoch 51 Step: 25025 Val loss: 1.5632107257843018\n",
            "Epoch 51 Step: 25050 Val loss: 1.5899306535720825\n",
            "Epoch 51 Step: 25075 Val loss: 1.5687209367752075\n",
            "Epoch 51 Step: 25100 Val loss: 1.5806931257247925\n",
            "Epoch 51 Step: 25125 Val loss: 1.548222541809082\n",
            "Epoch 51 Step: 25150 Val loss: 1.5552880764007568\n",
            "Epoch 51 Step: 25175 Val loss: 1.5865503549575806\n",
            "Epoch 51 Step: 25200 Val loss: 1.551100730895996\n",
            "Epoch 51 Step: 25225 Val loss: 1.5864520072937012\n",
            "Epoch 51 Step: 25250 Val loss: 1.5826666355133057\n",
            "Epoch 51 Step: 25275 Val loss: 1.5428506135940552\n",
            "Epoch 51 Step: 25300 Val loss: 1.5573629140853882\n",
            "Epoch 51 Step: 25325 Val loss: 1.5942295789718628\n",
            "Epoch 51 Step: 25350 Val loss: 1.5632888078689575\n",
            "Epoch 51 Step: 25375 Val loss: 1.5713858604431152\n",
            "Epoch 51 Step: 25400 Val loss: 1.5442049503326416\n",
            "Epoch 51 Step: 25425 Val loss: 1.5556057691574097\n",
            "Epoch 51 Step: 25450 Val loss: 1.565277099609375\n",
            "Epoch 51 Step: 25475 Val loss: 1.565183162689209\n",
            "Epoch 52 Step: 25500 Val loss: 1.5842787027359009\n",
            "Epoch 52 Step: 25525 Val loss: 1.5565659999847412\n",
            "Epoch 52 Step: 25550 Val loss: 1.5640218257904053\n",
            "Epoch 52 Step: 25575 Val loss: 1.5482231378555298\n",
            "Epoch 52 Step: 25600 Val loss: 1.594840168952942\n",
            "Epoch 52 Step: 25625 Val loss: 1.571712613105774\n",
            "Epoch 52 Step: 25650 Val loss: 1.548020601272583\n",
            "Epoch 52 Step: 25675 Val loss: 1.5849218368530273\n",
            "Epoch 52 Step: 25700 Val loss: 1.5831310749053955\n",
            "Epoch 52 Step: 25725 Val loss: 1.5476747751235962\n",
            "Epoch 52 Step: 25750 Val loss: 1.5430129766464233\n",
            "Epoch 52 Step: 25775 Val loss: 1.5707955360412598\n",
            "Epoch 52 Step: 25800 Val loss: 1.5834482908248901\n",
            "Epoch 52 Step: 25825 Val loss: 1.569242238998413\n",
            "Epoch 52 Step: 25850 Val loss: 1.5617743730545044\n",
            "Epoch 52 Step: 25875 Val loss: 1.573350191116333\n",
            "Epoch 52 Step: 25900 Val loss: 1.5521810054779053\n",
            "Epoch 52 Step: 25925 Val loss: 1.5718731880187988\n",
            "Epoch 52 Step: 25950 Val loss: 1.5773439407348633\n",
            "Epoch 53 Step: 25975 Val loss: 1.581468105316162\n",
            "Epoch 53 Step: 26000 Val loss: 1.5406876802444458\n",
            "Epoch 53 Step: 26025 Val loss: 1.5889501571655273\n",
            "Epoch 53 Step: 26050 Val loss: 1.5641536712646484\n",
            "Epoch 53 Step: 26075 Val loss: 1.5846530199050903\n",
            "Epoch 53 Step: 26100 Val loss: 1.5860943794250488\n",
            "Epoch 53 Step: 26125 Val loss: 1.5831952095031738\n",
            "Epoch 53 Step: 26150 Val loss: 1.5772666931152344\n",
            "Epoch 53 Step: 26175 Val loss: 1.6041191816329956\n",
            "Epoch 53 Step: 26200 Val loss: 1.5768790245056152\n",
            "Epoch 53 Step: 26225 Val loss: 1.5640921592712402\n",
            "Epoch 53 Step: 26250 Val loss: 1.5914273262023926\n",
            "Epoch 53 Step: 26275 Val loss: 1.5845868587493896\n",
            "Epoch 53 Step: 26300 Val loss: 1.5798457860946655\n",
            "Epoch 53 Step: 26325 Val loss: 1.581264853477478\n",
            "Epoch 53 Step: 26350 Val loss: 1.5857475996017456\n",
            "Epoch 53 Step: 26375 Val loss: 1.586667537689209\n",
            "Epoch 53 Step: 26400 Val loss: 1.5869232416152954\n",
            "Epoch 53 Step: 26425 Val loss: 1.5633249282836914\n",
            "Epoch 53 Step: 26450 Val loss: 1.5805062055587769\n",
            "Epoch 54 Step: 26475 Val loss: 1.554124355316162\n",
            "Epoch 54 Step: 26500 Val loss: 1.5532159805297852\n",
            "Epoch 54 Step: 26525 Val loss: 1.5889989137649536\n",
            "Epoch 54 Step: 26550 Val loss: 1.5572115182876587\n",
            "Epoch 54 Step: 26575 Val loss: 1.556894063949585\n",
            "Epoch 54 Step: 26600 Val loss: 1.57390558719635\n",
            "Epoch 54 Step: 26625 Val loss: 1.5776335000991821\n",
            "Epoch 54 Step: 26650 Val loss: 1.5664259195327759\n",
            "Epoch 54 Step: 26675 Val loss: 1.5857279300689697\n",
            "Epoch 54 Step: 26700 Val loss: 1.5868580341339111\n",
            "Epoch 54 Step: 26725 Val loss: 1.5614484548568726\n",
            "Epoch 54 Step: 26750 Val loss: 1.5910173654556274\n",
            "Epoch 54 Step: 26775 Val loss: 1.5753116607666016\n",
            "Epoch 54 Step: 26800 Val loss: 1.5440270900726318\n",
            "Epoch 54 Step: 26825 Val loss: 1.576788067817688\n",
            "Epoch 54 Step: 26850 Val loss: 1.5805690288543701\n",
            "Epoch 54 Step: 26875 Val loss: 1.5717393159866333\n",
            "Epoch 54 Step: 26900 Val loss: 1.5762739181518555\n",
            "Epoch 54 Step: 26925 Val loss: 1.5545684099197388\n",
            "Epoch 54 Step: 26950 Val loss: 1.5531280040740967\n",
            "Epoch 55 Step: 26975 Val loss: 1.5907330513000488\n",
            "Epoch 55 Step: 27000 Val loss: 1.5868548154830933\n",
            "Epoch 55 Step: 27025 Val loss: 1.6090878248214722\n",
            "Epoch 55 Step: 27050 Val loss: 1.5473037958145142\n",
            "Epoch 55 Step: 27075 Val loss: 1.5566277503967285\n",
            "Epoch 55 Step: 27100 Val loss: 1.5469642877578735\n",
            "Epoch 55 Step: 27125 Val loss: 1.5655039548873901\n",
            "Epoch 55 Step: 27150 Val loss: 1.5722808837890625\n",
            "Epoch 55 Step: 27175 Val loss: 1.557329535484314\n",
            "Epoch 55 Step: 27200 Val loss: 1.5561790466308594\n",
            "Epoch 55 Step: 27225 Val loss: 1.585067629814148\n",
            "Epoch 55 Step: 27250 Val loss: 1.560179591178894\n",
            "Epoch 55 Step: 27275 Val loss: 1.5890928506851196\n",
            "Epoch 55 Step: 27300 Val loss: 1.5745313167572021\n",
            "Epoch 55 Step: 27325 Val loss: 1.5692622661590576\n",
            "Epoch 55 Step: 27350 Val loss: 1.5594890117645264\n",
            "Epoch 55 Step: 27375 Val loss: 1.558329463005066\n",
            "Epoch 55 Step: 27400 Val loss: 1.6043835878372192\n",
            "Epoch 55 Step: 27425 Val loss: 1.5584723949432373\n",
            "Epoch 56 Step: 27450 Val loss: 1.5418436527252197\n",
            "Epoch 56 Step: 27475 Val loss: 1.574364185333252\n",
            "Epoch 56 Step: 27500 Val loss: 1.5952402353286743\n",
            "Epoch 56 Step: 27525 Val loss: 1.5719563961029053\n",
            "Epoch 56 Step: 27550 Val loss: 1.5804322957992554\n",
            "Epoch 56 Step: 27575 Val loss: 1.56978178024292\n",
            "Epoch 56 Step: 27600 Val loss: 1.5718330144882202\n",
            "Epoch 56 Step: 27625 Val loss: 1.593367099761963\n",
            "Epoch 56 Step: 27650 Val loss: 1.5718512535095215\n",
            "Epoch 56 Step: 27675 Val loss: 1.611586570739746\n",
            "Epoch 56 Step: 27700 Val loss: 1.5806163549423218\n",
            "Epoch 56 Step: 27725 Val loss: 1.5488966703414917\n",
            "Epoch 56 Step: 27750 Val loss: 1.5718022584915161\n",
            "Epoch 56 Step: 27775 Val loss: 1.5987523794174194\n",
            "Epoch 56 Step: 27800 Val loss: 1.5765106678009033\n",
            "Epoch 56 Step: 27825 Val loss: 1.5784540176391602\n",
            "Epoch 56 Step: 27850 Val loss: 1.5669505596160889\n",
            "Epoch 56 Step: 27875 Val loss: 1.5794472694396973\n",
            "Epoch 56 Step: 27900 Val loss: 1.5789978504180908\n",
            "Epoch 56 Step: 27925 Val loss: 1.5675928592681885\n",
            "Epoch 57 Step: 27950 Val loss: 1.5827267169952393\n",
            "Epoch 57 Step: 27975 Val loss: 1.5553462505340576\n",
            "Epoch 57 Step: 28000 Val loss: 1.564727783203125\n",
            "Epoch 57 Step: 28025 Val loss: 1.5618586540222168\n",
            "Epoch 57 Step: 28050 Val loss: 1.600744605064392\n",
            "Epoch 57 Step: 28075 Val loss: 1.5765280723571777\n",
            "Epoch 57 Step: 28100 Val loss: 1.544425368309021\n",
            "Epoch 57 Step: 28125 Val loss: 1.5767194032669067\n",
            "Epoch 57 Step: 28150 Val loss: 1.5832128524780273\n",
            "Epoch 57 Step: 28175 Val loss: 1.5847392082214355\n",
            "Epoch 57 Step: 28200 Val loss: 1.5513137578964233\n",
            "Epoch 57 Step: 28225 Val loss: 1.5727245807647705\n",
            "Epoch 57 Step: 28250 Val loss: 1.5797748565673828\n",
            "Epoch 57 Step: 28275 Val loss: 1.5685663223266602\n",
            "Epoch 57 Step: 28300 Val loss: 1.5586098432540894\n",
            "Epoch 57 Step: 28325 Val loss: 1.5777698755264282\n",
            "Epoch 57 Step: 28350 Val loss: 1.5523356199264526\n",
            "Epoch 57 Step: 28375 Val loss: 1.6044033765792847\n",
            "Epoch 57 Step: 28400 Val loss: 1.5795363187789917\n",
            "Epoch 58 Step: 28425 Val loss: 1.5788668394088745\n",
            "Epoch 58 Step: 28450 Val loss: 1.5581012964248657\n",
            "Epoch 58 Step: 28475 Val loss: 1.6048691272735596\n",
            "Epoch 58 Step: 28500 Val loss: 1.5667818784713745\n",
            "Epoch 58 Step: 28525 Val loss: 1.5945814847946167\n",
            "Epoch 58 Step: 28550 Val loss: 1.5947295427322388\n",
            "Epoch 58 Step: 28575 Val loss: 1.5913773775100708\n",
            "Epoch 58 Step: 28600 Val loss: 1.590345859527588\n",
            "Epoch 58 Step: 28625 Val loss: 1.5691556930541992\n",
            "Epoch 58 Step: 28650 Val loss: 1.5689435005187988\n",
            "Epoch 58 Step: 28675 Val loss: 1.5816307067871094\n",
            "Epoch 58 Step: 28700 Val loss: 1.5893532037734985\n",
            "Epoch 58 Step: 28725 Val loss: 1.6037057638168335\n",
            "Epoch 58 Step: 28750 Val loss: 1.5823493003845215\n",
            "Epoch 58 Step: 28775 Val loss: 1.5912487506866455\n",
            "Epoch 58 Step: 28800 Val loss: 1.5878944396972656\n",
            "Epoch 58 Step: 28825 Val loss: 1.5984090566635132\n",
            "Epoch 58 Step: 28850 Val loss: 1.5813660621643066\n",
            "Epoch 58 Step: 28875 Val loss: 1.5625497102737427\n",
            "Epoch 58 Step: 28900 Val loss: 1.5827476978302002\n",
            "Epoch 59 Step: 28925 Val loss: 1.5606334209442139\n",
            "Epoch 59 Step: 28950 Val loss: 1.5591062307357788\n",
            "Epoch 59 Step: 28975 Val loss: 1.601615071296692\n",
            "Epoch 59 Step: 29000 Val loss: 1.5616631507873535\n",
            "Epoch 59 Step: 29025 Val loss: 1.5763245820999146\n",
            "Epoch 59 Step: 29050 Val loss: 1.5797839164733887\n",
            "Epoch 59 Step: 29075 Val loss: 1.5875988006591797\n",
            "Epoch 59 Step: 29100 Val loss: 1.5677258968353271\n",
            "Epoch 59 Step: 29125 Val loss: 1.579750657081604\n",
            "Epoch 59 Step: 29150 Val loss: 1.5878981351852417\n",
            "Epoch 59 Step: 29175 Val loss: 1.5608350038528442\n",
            "Epoch 59 Step: 29200 Val loss: 1.5339703559875488\n",
            "Epoch 59 Step: 29225 Val loss: 1.5379812717437744\n",
            "Epoch 59 Step: 29250 Val loss: 1.5200557708740234\n",
            "Epoch 59 Step: 29275 Val loss: 1.5394293069839478\n",
            "Epoch 59 Step: 29300 Val loss: 1.5631593465805054\n",
            "Epoch 59 Step: 29325 Val loss: 1.554011344909668\n",
            "Epoch 59 Step: 29350 Val loss: 1.5724763870239258\n",
            "Epoch 59 Step: 29375 Val loss: 1.541090726852417\n",
            "Epoch 59 Step: 29400 Val loss: 1.5503073930740356\n"
          ]
        }
      ],
      "source": [
        "model.train()\n",
        "\n",
        "if model.use_gpu:\n",
        "  model.cuda()\n",
        "\n",
        "for i in range(epochs):\n",
        "\n",
        "  hidden = model.hidden_state(batch_size)\n",
        "\n",
        "  for x, y in generate_batches(train_data, batch_size, seq_len):\n",
        "    tracker += 1\n",
        "\n",
        "    x = one_hot_encode(x, num_char)\n",
        "    inputs = torch.from_numpy(x)\n",
        "    targets = torch.from_numpy(y)\n",
        "\n",
        "    if model.use_gpu:\n",
        "      inputs = inputs.cuda()\n",
        "      targets = targets.cuda()\n",
        "\n",
        "    hidden = tuple([state.data for state in hidden])\n",
        "    model.zero_grad()\n",
        "\n",
        "    lstm_out, hidden = model.forward(inputs, hidden)\n",
        "    loss = criterion(lstm_out, targets.reshape(batch_size*seq_len).long())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
        "    optim.step()\n",
        "\n",
        "    if tracker % 25 == 0:\n",
        "      val_hidden = model.hidden_state(batch_size)\n",
        "      val_losses = []\n",
        "\n",
        "      model.eval()\n",
        "      for x, y in generate_batches(validation_data, batch_size, seq_len):\n",
        "        x = one_hot_encode(x, num_char)\n",
        "        inputs = torch.from_numpy(x)\n",
        "        targets = torch.from_numpy(y)\n",
        "\n",
        "        if model.use_gpu:\n",
        "          inputs = inputs.cuda()\n",
        "          targets = targets.cuda()\n",
        "\n",
        "        val_hidden = tuple([state.data for state in hidden])\n",
        "        lstm_out, val_hidden = model.forward(inputs, val_hidden)\n",
        "        val_loss = criterion(lstm_out, targets.reshape(batch_size*seq_len).long())\n",
        "\n",
        "        val_losses.append(val_loss.item())\n",
        "\n",
        "      model.train()\n",
        "      print(f\"Epoch {i} Step: {tracker} Val loss: {val_loss.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KX5TVJWA94mA"
      },
      "outputs": [],
      "source": [
        "model_name = \"hidden512_layers3_shakes.net\"\n",
        "torch.save(model.state_dict(), model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMXgAvxJ-huu"
      },
      "outputs": [],
      "source": [
        "def predict_next_character(model, char, hidden=None, k=1):\n",
        "\n",
        "  encoded_text = model.encoder[char]\n",
        "  encoded_text = np.array([[encoded_text]])\n",
        "  encoded_text = one_hot_encode(encoded_text, len(model.all_chars))\n",
        "  inputs = torch.from_numpy(encoded_text)\n",
        "\n",
        "  if model.use_gpu:\n",
        "    inputs = inputs.cuda()\n",
        "\n",
        "  hidden = tuple([state.data for state in hidden])\n",
        "  lstm_out, hidden = model(inputs, hidden)\n",
        "  probs = F.softmax(lstm_out, dim=1).data\n",
        "\n",
        "  if model.use_gpu:\n",
        "    probs = probs.cpu()\n",
        "\n",
        "  probs, index_positions = probs.topk(k)\n",
        "  index_positions = index_positions.numpy().squeeze()\n",
        "\n",
        "  probs = probs.numpy().flatten()\n",
        "  probs = probs/probs.sum()\n",
        "  char = np.random.choice(index_positions, p=probs)\n",
        "\n",
        "  return model.decoder[char], hidden\n",
        "\n",
        "\n",
        "def generate_text(module, size, seed=\"The\", k=1):\n",
        "  if model.use_gpu():\n",
        "    model.cuda()\n",
        "\n",
        "  else:\n",
        "    model.cpu()\n",
        "\n",
        "  model.eval()\n",
        "  output_chars = [c for c in seed]\n",
        "\n",
        "  hidden = model.hidden_state(1)\n",
        "  for char in seed:\n",
        "    char, hidden = predict_next_character(model, char, hidden, k)\n",
        "\n",
        "  output_chars.append(char)\n",
        "\n",
        "  for i in range(size):\n",
        "    char, hidden = predict_next_character(model, output_chars[-1], hidden, k)\n",
        "    output_chars.append(char)\n",
        "\n",
        "  return ''.join(output_chars)\n",
        "\n",
        "print(generate_text(model, 1000, seed=\"The\", k=3))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8rwgYtftrHm/cuaL8HVkh",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}